<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<link rel="stylesheet" type="text/css" href="default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="index.html">Home</a>
				<a href="event.html">News</a>
				<a href="people.html">People</a>
				<a href="robots.html">Robots</a>
				<a href="research.html">Research</a>
				<a href="publications.html">Publications</a>
				<a href="courses.html">Education</a>
			</div>

			<div class="main">
				<h0>Robotics, Vision, Mobility, Manipulation</h0>
				<p></p>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-duckieboat22.png" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Duckiepond 2.0: an Education and Research Environment of Reinforcement Learning-based Navigation and Virtual Reality Human Interactions for a Heterogenous Maritime Fleet
						</h1>
						<p>
							Duckiepond 2.0 extends the education and research environment Duckiepond, including an accessible “Duckieboat” platform 
							and simulation environments (Gazebo and Unity). Duckieboat (DBT22) is built upon commercially available inflatable boats 
							and outboard motors. The modularity designs of sensor tower, autonomy box, and communication module (DuckieAnchor) can be 
							deployed on heterogeneous autonomous surface vehicles, providing longer communication ranges and hardware-in-the-loop (HIL) 
							developments. Duckiepond 2.0 inherited the recent efforts of the Duckietown platform for flexible fleet managements, such 
							as the Duckietown Shell commands. Duckiepond 2.0 continues the supports of MOOS-ROS bridge, and further develop PyIvP 
							(Python binding of the IvP C++ code) and non-ROS WebSocket as autonomy education materials for wider uses of the communities. 
							The Gazebo simulation provided by Virtual RobotX (VRX) consists of physical engines of realistic vehicle dynamics and high 
							fidelitous current, wind and sensors streams, which are used to develop deep reinforcement learning algorithms of collision 
							avoidance, navigation, and docking. Lastly, Duckiepond 2.0 considers “supervised autonomy” by including virtual reality (VR) 
							interactions with a human supervisor at base station. The proposed VR interface is based on a consumer-grade device 
							(Oculus Quest 2) with one or more Duckieboats to allow easy replication and set up. Virtual representations of the Duckieboats 
							and surrounding objects in the real-world presented in VR can be used by a single human supervisor to control the entire heterogenous 
							maritime fleet for search and rescue missions. 

						</p>
						<a href="projects/duckiepond.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>
				
				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-pokingbot-new.png" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Curriculum Reinforcement Learning from Avoiding Collisions to Navigating among Movable Obstacles in Diverse Environments
						</h1>
						<p>
							Curriculum learning has proven highly effective to speed of convergence with improved performance 
							in a variety of applications. However, curriculum generation requires the ranking of sub-tasks in 
							order of difficulty, which may depend on application domains. Deep reinforcement learning (RL)-based 
							navigation typically involves collision avoidance and the planning of drivable paths towards a desired goal. 
							Existing methods based solely on navigation are unable to deal with blocked pathways, thereby navigation among 
							movable obstacles (NAMO) involves more complex controls in continuous action space.
							Researchers dealing with robot navigation problems have yet to devise effective methods to rank samples 
							from easy to hard or devise a suitable pacing function in diverse unseen environments. 
							In the current study, we ranked the navigation difficulty metrics of various large-scale representative 
							environments and trained DRL policies from scratch within a certain computation budgets. 
							We found that low difficulty environments received high rewards, in particular in a relatively open tunnel-like 
							environment that only required wall following. To facilitate more complex policies in NAMO task, we leveraged curriculum learning 
							built upon pre-trained policies, and developed pace functions appropriate to the difficulty of the environment. 
							The proposed scheme proved highly effective to train a local planner capable of clearing the path of movable obstacles. 
							Comprehensive evaluations were assessed in experiments conducted in simulated and real environments . 
						</p>
						<a href="projects/curl-navi.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/vr-teaser.JPG" width="210" border="0">
					</div>
					<div class="proj">
						<h1>WFH-VR: Teleoperating a Robot Arm to set a Dining Table across the Globe via Virtual Reality</h1>
						<p>
							This paper presents an easy-to-deploy, virtual reality-based teleoperation system for controlling a robot arm. 
							The proposed system is based on a consumer-grade virtual reality device (Oculus Quest 2) with a low-cost robot arm (a LoCoBot) to allow easy replication and set up. 
							The proposed Work-from-Home Virtual Reality (WFH-VR) system allows the user to feel an intimate connection with the real remote robot arm. 
							Virtual representations of the robot and objects to be manipulated in the real-world are presented in VR by streaming data pertaining to orientation and poses.
							The user studies suggest that 1) the proposed telerobotic system is effective under conditions both with and without network latency, whereas a method that simply streams video does not. 
							This design enables the system implemented at an arbitrary distance from the actual work site. 2) The proposed system allows novices to perform manipulation tasks requiring higher dexterity than traditional keyboard controls can support, such as setting tableware.
						</p>
						<a href="projects/vr-robot-arm.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/handover_smart_health.png" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Fed-HANet: Federated Visual Grasping Learning for Human Robot Handovers</h1>
						<p>
							Human-to-robot (H2R) handover is a key capability of service robots, but few of recent works have addressed the challenge at the diverse and unknown objects.  
							Although end-to-end grasping directly has made tremendous progress at agnostic object in logistic tasks, most grasping algorithms are limited to top-down movements.
							Due to the variable preference on holding and delivering by the operators, it has not been mapped to H2R. 
							We present an approach for end-to-end reactive 6DoF grasp choice by inferring dense pixel-wise probability maps of the affordances with multiple views. 
							Besides of closed-loop real-time motion planning, our system not only hand over the object with suitable grasping angles, but also allows the robot arm to grasp the object via the narrowest side of the objects. 
							Hence, our system can adapt to user' preference and a wider range of object categories with the same gripper. 
							In addition, our approach can also apply to federated learning framework to keep our personal data private. 
							We compare our results of evaluation to 3 state-of-the-art approaches and verify ours on 4 novel benchmark sets of the objects. 
							Furthermore, we demonstrate the effectiveness through 2 user study with 12 participants respectively. 
							The dataset for training and all pretrained models are open source.
						</p>
						<a href="projects/fed-hanet.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-guiding.jpeg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Assistive Navigation using Deep Reinforcement Learning Guiding Robot with UWB Beacons and Semantic Feedbacks for Blind and Visually Impaired People</h1>
						<p>
							Facilitating navigation in pedestrian environments is critical for enabling people who are blind and visually impaired (BVI) to achieve independent mobility. A deep-reinforcement-learning-based assistive guiding robot with ultrawide-bandwidth (UWB) beacons that can navigate through routes with designated waypoints was designed in this study. Typically, a simultaneous localization and mapping (SLAM) framework is used to estimate the robot pose and navigational goal; however, SLAM frameworks are vulnerable in certain dynamic environments. The proposed navigation method is a learning approach based on state-of-the-art deep reinforcement learning and can effectively avoid objects. When used with UWB beacons, the proposed strategy is suitable for environments with dynamic pedestrians. We also designed a harness device with an audio interface that enables BVI users to interact with the guiding robot through intuitive feedback. The UWB beacons were installed with an audio interface to obtain environmental information The on-harness and on-beacon verbal feedback provides information on points-of-interest (POI) and turn-by-turn information to BVI users. BVI users were recruited in this study to conduct navigation tasks in different scenarios. A route was designed in a simulated ward to represent daily activities. In real-world situations, SLAM-based state estimation might be affected by dynamic obstacles, and the visual-based trail may suffer from occlusions from pedestrians or other obstacles. The proposed system successfully navigated through environments with dynamic pedestrians, in which systems based on existing SLAM algorithms have failed.
						</p>
						<a href="projects/guiding_robot_uwb.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-mmrl.png" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Cross-Modal Contrastive Learning of Representations for Navigation</h1>
						<h2>using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions</h2>
						<p>
					We propose to enable learning-based autonomous navigation with
					single-chip lightweight, low-cost millimeter-wave (mmWave) radars. Given that the mmWave
					radar signals are often noisy and sparse, we proposed cross-modal contrastive learning for
					representation (CM-CLR) by maximizing the agreements between mmWave radar and LiDAR signals
					in training stage. We carried out real robot quantitative evaluations, and compared with 1)
					a method with two separate networks using cross-modal generative reconstruction and RL
					policy, and 2) a baseline RL policy without cross-modal representation. We showed that the
					proposed end-to-end deep RL policy with contrastive learning successfully navigate the robot
					through smoke-filled maze environments, and demonstrated superior performance than
					generative reconstruction that sometimes produced noise artifact walls or obstacles.
						</p>
						<a href="projects/deeprl-mmWave.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/images/team_nctu.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>A Heterogeneous Unmanned Ground Vehicle and
							Blimp Robot Team for Search and Rescue using
							Data-driven Autonomy and Communication-aware
							Navigation
						</h1>
						<h2>DARPA Subterranean Challengen</h2>
						<p>
							We joined DARPA Subterranean Challenge as Team NCTU. We developed an architecture and
							implement heterogeneous unmanned ground vehicles (UGVs) and blimp robot teams to navigate
							unknown terrains in subterranean environments for search and rescue missions.
						</p>
						<a href="SubT/index.html" target="_blank">Project Link</a>
						<a href="projects/team_nctu_subt.html" target="_blank">Field Robotiscs paper link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/duckiepond.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Duckiepond</h1>
						<p>
							Duckiepond is an education and research development environment that includes software
							systems, educational materials, and of a fleet of autonomous surface vehicles Duckieboat.
							Duckieboats are designed to be easily reproducible with parts from a 3D printer and other
							commercially available parts, with flexible software that leverages several open source
							packages.
						</p>
						<a href="https://robotx-nctu.github.io/duckiepond" target="_blank">Project Link</a>
						<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
						<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/brandname.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Brandname-based Manipulation</h1>
						<p>
							We develop a pose-aware placement approach by spotting the semantic labels (e.g.,
							brandnames) of objects in a cluttered tote and then carrying out a sequence of actions to
							place the objects on a shelf or on a conveyor with desired poses. Our major contributions
							include 1) providing an open benchmark dataset of objects and brandnames with multi-view
							segmentation for training and evaluations; 2) carrying out comprehensive evaluations for our
							brandname-based fully convolutional network (FCN) that can predict the affordance and grasp
							to achieve pose-aware placement, whose success rates decrease along with clutters; 3)
							showing that active manipulation with two cooperative manipulators and grippers can
							effectively handle the occlusion of brandnames.
							<a href="https://text-pick-n-place.github.io/TextPNP/" target="_blank">Project Link</a>
							<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
							<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
						</p>
					</div>
				</div>


				<div class="fullcontent">
					<div class="teaser">
						<img src="research/robotic_guide_dog.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Robotic Guide Dog Project</h1>
						<p>
							We develop the robotic guide dog which used a vision-based learning approach to autonomously
							follow man-made trails on various terrains of general pedestrian environments to enable
							independent navigation for people who are blind or visually impaired (BVI).<br>
							<a href="projects/robotic_guide_dog.html" target="_blank">Project Link</a>
							<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
							<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/duckietown.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Duckietown: an Open, Inexpensive and Flexible Platform for Autonomy Education and Research
						</h1>
						<p>
							In Duckietown, inhabitants (duckies) are transported via an autonomous mobility service
							(Duckiebots). Duckietown is designed to be inexpensive and modular, yet still enable many of
							the research and educational opportunities of a full-scale self-driving car platform.<br>
							<a href="https://duckietown-nctu.github.io/" target="_blank">Project Link</a> <br>
							<a href="summer_school.html" target="_blank">Duckietown Summer School in 2017</a> <br>
						</p>
					</div>
				</div>

				<!--20180306 samliu end-->

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/bocelli-project-teaser.gif" width="210" height="100" border="0">
					</div>
					<div class="proj">
						<h1>Wearable Navigation System for the Blind and Visually Impaired</h1>
						<p>
							Our team develops wearable, miniaturized, energy-efficient, and soft robotic systems
							to provide key vision functionalities and navigation feedbacks for blind and visually
							impaired people.
							For safe mobility, we develop methods for independent awareness of obstacles, drop-offs,
							ascents, descents,
							and also important objects or landmarks.
							A haptic array and rapidly refreshable Braille device are used to deliver information about
							the surroundings.
							<a href="research/CPS_NickWang_poster_jpg.pdf" target="_blank">Poster</a>
							<a href="http://projects.csail.mit.edu/bocelli/" target="_blank">Website (2014)</a>
						</p>
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
						<img src="research/robotx_placard.gif" height="120" width="200" />
					</div>
					<div class="proj">
						<h1>RobotX Competition</h1>
						<p>
							The inaugural RobotX competition was held in Singapore in Oct. 2014.
							The purpose of the competition was to challenge teams to develop new strategies for tackling
							unique and
							important problems in marine robotics. Placard detection is one of the tasks, and our key
							design objectives were:
							1. Robustness to degradation caused by motion, scale and perspective transformation from
							different viewing positions,
							warp and occlusion caused by wind, and variants of color from light condition, 2. Speed and
							accuracy to support real-time decision-making.
							Ultimately the MIT/Olin team narrowly <strong>won first place</strong> in a competitive
							field.
							<a href="publications/fsr2015_robotx.pdf" target="_blank">Paper</a>
							<a href="http://newsoffice.mit.edu/2014/csail-meche-marine-experts-win-robotx-self-driving-boat-competition-1030"
								target="_blank">MIT News</a>
						</p>
					</div>
					<div class="fullcontent">
						<div class="teaser">
							<img src="research/multiple_observation_integration.gif" height="160" width="200" />
						</div>
						<div class="proj">
							<h1>Spatially Prioritized and Persistent Text Detection and Decoding</h1>
							<p>
								Our method uses simultaneous
								localization and mapping (SLAM) to extract planar “tiles” repre-
								senting scene surfaces.
								This paper’s contributions include: 1) spatiotemporal fusion
								of tile observations via SLAM, prior to inspection, thereby
								improving the quality of the input data; and 2) combination of
								multiple noisy text observations into a single higher-confidence
								estimate of environmental text.
								<a href="publications/Wang_cbdar2013.pdf" target="_blank">Paper</a>
							</p>
						</div>
					</div>

				</div>
				<h0>Visual Attention and Scene Perception</h0>
				<p></p>
				<div class="fullcontent">
					<div class="teaser">
						<p></p>
						<img src="research/jov_text_attraction.gif" width="130" height="130" border="0">
					</div>
					<div class="proj">
						<h1>The Attraction of Visual Attention to Texts in Real-World Scenes</h1>
						<h2><em>Where can you more likely find text?</em></h2>
						<p>
							The present study was aimed at investigating its underlying factors of human visual
							attention attracted by texts.
							The following main results were obtained: (a) Greater fixation probability and
							shorter minimum fixation distance of texts confirmed the higher attractiveness of texts; (b)
							the locations where texts are
							typically placed contribute partially to this effect; (c) specific visual features of texts,
							rather than typically salient features
							(e.g., color, orientation, and contrast), are the main attractors of attention; (d) the
							meaningfulness of texts does not add to
							their attentional capture; and (e) the attraction of attention depends to some extent on the
							observer’s familiarity with the
							writing system and language of a given text.
							<a href="publications/WangPomplun2012.pdf" target="_blank"> Paper</a>
						</p>
					</div>
				</div>
				<h0>Cognitive Models and Reading</h0>
				<p></p>
				<div class="fullcontent">
					<div class="teaser">
						<img src="research/brm.gif" width="210" height="200" border="0">
					</div>
					<div class="proj">
						<h1>Predicting Semantic Transparency Judgments</h1>
						<h2><em>Do you read “butter” when you read “butterfly”? Or “馬” for “馬虎”?</em></h2>
						The morphological constituents of English compounds and two-character Chinese compounds may
						differ in meaning from the whole word.
						The proposed models successfully predicted participants’ transparency ratings, and the
						prediction is influenced by the differences in raters’ morphological processing
						in the different writing systems. The dominance of lexical meaning, semantic transparency, and
						the average similarity between all pairs within a morphological family
						are provided, and practical applications for future studies are discussed.
						<a href="publications/Wang2014_brm.pdf" target="_blank">Paper</a>
						<a href="http://www.lsa.url.tw/modules/lsa/" target="_blank">Website</a>
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
						<p></p>
						<img src="research/chinese_svd.gif" width="210" height="160" border="0">
						<br><br><br>
					</div>
					<div class="proj">
						<h1>Degraded Character Recognition</h1>
						<h2><em>Can you read these sentence?</em></h2>
						<p>
							It is known that not all letters are of equal importance to the word recognition process,
							i.e., changing initial or exterior letters are more disruptchanging.
							Consistent results have been found for Chinese characters, and first-written strokes are
							more important for reading.
							This study compares the effects of high-level learnt stroke writting order and low-level
							features obtained via pixel-level singular value decompostion (SVD)
							to readers' degraded character recognition. Our results suggest that the most important
							segments determined by SVD, which has no information about stroke writing order, can
							identify first-written strokes. Furthermore, our results suggest that contour may be
							correlated with stroke writing order.
							<a href="publications/Wang2013-jrir.pdf" target="_blank"> Paper</a>
							<a href="publications/CICEM_Talk_NickWang.pdf" target="_blank">Slides</a>
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
					</div>
					<div class="proj">
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
					</div>
					<div class="proj">
					</div>

				</div>
				<div class="clearer">&nbsp;</div>

			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span>

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
