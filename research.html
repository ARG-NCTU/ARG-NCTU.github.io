<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<link rel="stylesheet" type="text/css" href="default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="index.html">Home</a>
				<a href="event.html">News</a>
				<a href="people.html">People</a>
				<a href="robots.html">Robots</a>
				<a href="research.html">Research</a>
				<a href="publications.html">Publications</a>
				<a href="courses.html">Education</a>
			</div>

			<div class="main">
				<h0>Robotics, Vision, Mobility, Manipulation</h0>
				<p></p>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-pokingbot.png" width="210" border="0">
					</div>
					<div class="proj">
						<h1>PokingBot: Enabling Real-world Interactive Navigation via Curriculum Reinforcement Learning for Search and Rescue Missions
						</h1>
						<p>
							Robot navigation is typically carried out by avoiding obstacles
							and planing drivable paths towards a desired
							goal. However, existing navigation-only methods are limited to
							perceive without interacting with movable obstacles. Interactive
							navigation with mobile manipulators has been recently addressed
							using deep reinforcement learning (RL). However, existing RL
							approaches still are limited to simulation results. The challenges
							of interactive navigation in real robot and environments were
							still rather unexplored. Inspired by the DARPA Subterranean
							(SubT) Challenge we tackle the situations while semi-dynamic
							but movable obstacles block the way. In this paper 1) we design
							a full mobile manipulation system PokingBot for search and
							rescue missions with a DRL-based local planner of interactive
							navigation. 2) We argue that by leveraging curriculum learning
							settings of both avoid and interact with obstacles, our agent learns
							to efficiently select a policy conducive to discovering high-reward
							sequential continuous actions. 3) Comprehensive large-scale simulation in the SubT Urban Circuit Nuclear Power Plant and
							a proof-of-concept real-world corridor experiments are carried
							out against a state-of-the-art planner.
						</p>
						<a href="projects/drl-transfer.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/vr-teaser.JPG" width="210" border="0">
					</div>
					<div class="proj">
						<h1>WFH-VR: Teleoperating a Robot Arm to set a Dining Table across the Globe via Virtual Reality</h1>
						<p>
							his paper presents an easy-to-deploy, virtual reality-based teleoperation system for controlling a robot arm. 
							The proposed system is based on a consumer-grade virtual reality device (Oculus Quest 2) with a low-cost robot arm (a LoCoBot) to allow easy replication and set up. 
							The proposed Work-from-Home Virtual Reality (WFH-VR) system allows the user to feel an intimate connection with the real remote robot arm. 
							Virtual representations of the robot and objects to be manipulated in the real-world are presented in VR by streaming data pertaining to orientation and poses.
							The user studies suggest that 1) the proposed telerobotic system is effective under conditions both with and without network latency, whereas a method that simply streams video does not. 
							This design enables the system implemented at an arbitrary distance from the actual work site. 2) The proposed system allows novices to perform manipulation tasks requiring higher dexterity than traditional keyboard controls can support, such as setting tableware.
						</p>
						<a href="projects/vr-robot-arm.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/medical_teaser.PNG" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Learning End-­to­-End 6DoF Grasp Choice of Human-­to­-Robot Handover using Affordance Prediction and Deep Reinforcement Learning</h1>
						<p>
							We present our end-to-end 6DoF grasp choice for human-to-robot handover. We first leverage existing end-to-end grasping for the network backbone, and then finetune for preferred grasps using deep reinforcement learning. Comprehensive evaluations are carried out against various baselines using multi-stage hand and object prediction and subsequent planning. We show that the proposed approach was more robust to partial occlusions, and executed human preferred 6DoF grasps without hard-coding the correspondence of hand grasp classification.
						</p>
						<a href="projects/socially-aware-handover.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-guiding.jpeg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Assistive Navigation using Deep Reinforcement Learning Guiding Robot with UWB Beacons and Semantic Feedbacks for Blind and Visually Impaired People</h1>
						<p>
							Facilitating navigation in pedestrian environments is critical for enabling people who are blind and visually impaired (BVI) to achieve independent mobility. A deep-reinforcement-learning-based assistive guiding robot with ultrawide-bandwidth (UWB) beacons that can navigate through routes with designated waypoints was designed in this study. Typically, a simultaneous localization and mapping (SLAM) framework is used to estimate the robot pose and navigational goal; however, SLAM frameworks are vulnerable in certain dynamic environments. The proposed navigation method is a learning approach based on state-of-the-art deep reinforcement learning and can effectively avoid objects. When used with UWB beacons, the proposed strategy is suitable for environments with dynamic pedestrians. We also designed a harness device with an audio interface that enables BVI users to interact with the guiding robot through intuitive feedback. The UWB beacons were installed with an audio interface to obtain environmental information The on-harness and on-beacon verbal feedback provides information on points-of-interest (POI) and turn-by-turn information to BVI users. BVI users were recruited in this study to conduct navigation tasks in different scenarios. A route was designed in a simulated ward to represent daily activities. In real-world situations, SLAM-based state estimation might be affected by dynamic obstacles, and the visual-based trail may suffer from occlusions from pedestrians or other obstacles. The proposed system successfully navigated through environments with dynamic pedestrians, in which systems based on existing SLAM algorithms have failed.
						</p>
						<a href="projects/guiding_robot_uwb.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/img/teaser-mmrl.png" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Cross-Modal Contrastive Learning of Representations for Navigation</h1>
						<h2>using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions</h2>
						<p>
					We propose to enable learning-based autonomous navigation with
					single-chip lightweight, low-cost millimeter-wave (mmWave) radars. Given that the mmWave
					radar signals are often noisy and sparse, we proposed cross-modal contrastive learning for
					representation (CM-CLR) by maximizing the agreements between mmWave radar and LiDAR signals
					in training stage. We carried out real robot quantitative evaluations, and compared with 1)
					a method with two separate networks using cross-modal generative reconstruction and RL
					policy, and 2) a baseline RL policy without cross-modal representation. We showed that the
					proposed end-to-end deep RL policy with contrastive learning successfully navigate the robot
					through smoke-filled maze environments, and demonstrated superior performance than
					generative reconstruction that sometimes produced noise artifact walls or obstacles.
						</p>
						<a href="projects/deeprl-mmWave.html" target="_blank">Project Link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="projects/images/team_nctu.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>A Heterogeneous Unmanned Ground Vehicle and
							Blimp Robot Team for Search and Rescue using
							Data-driven Autonomy and Communication-aware
							Navigation
						</h1>
						<h2>DARPA Subterranean Challengen</h2>
						<p>
							We joined DARPA Subterranean Challenge as Team NCTU. We developed an architecture and
							implement heterogeneous unmanned ground vehicles (UGVs) and blimp robot teams to navigate
							unknown terrains in subterranean environments for search and rescue missions.
						</p>
						<a href="SubT/index.html" target="_blank">Project Link</a>
						<a href="projects/team_nctu_subt.html" target="_blank">Field Robotiscs paper link</a>
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/duckiepond.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Duckiepond</h1>
						<p>
							Duckiepond is an education and research development environment that includes software
							systems, educational materials, and of a fleet of autonomous surface vehicles Duckieboat.
							Duckieboats are designed to be easily reproducible with parts from a 3D printer and other
							commercially available parts, with flexible software that leverages several open source
							packages.
						</p>
						<a href="https://robotx-nctu.github.io/duckiepond" target="_blank">Project Link</a>
						<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
						<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/brandname.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Brandname-based Manipulation</h1>
						<p>
							We develop a pose-aware placement approach by spotting the semantic labels (e.g.,
							brandnames) of objects in a cluttered tote and then carrying out a sequence of actions to
							place the objects on a shelf or on a conveyor with desired poses. Our major contributions
							include 1) providing an open benchmark dataset of objects and brandnames with multi-view
							segmentation for training and evaluations; 2) carrying out comprehensive evaluations for our
							brandname-based fully convolutional network (FCN) that can predict the affordance and grasp
							to achieve pose-aware placement, whose success rates decrease along with clutters; 3)
							showing that active manipulation with two cooperative manipulators and grippers can
							effectively handle the occlusion of brandnames.
							<a href="https://text-pick-n-place.github.io/TextPNP/" target="_blank">Project Link</a>
							<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
							<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
						</p>
					</div>
				</div>


				<div class="fullcontent">
					<div class="teaser">
						<img src="research/robotic_guide_dog.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Robotic Guide Dog Project</h1>
						<p>
							We develop the robotic guide dog which used a vision-based learning approach to autonomously
							follow man-made trails on various terrains of general pedestrian environments to enable
							independent navigation for people who are blind or visually impaired (BVI).<br>
							<a href="projects/robotic_guide_dog.html" target="_blank">Project Link</a>
							<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
							<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
						</p>
					</div>
				</div>

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/duckietown.jpg" width="210" border="0">
					</div>
					<div class="proj">
						<h1>Duckietown: an Open, Inexpensive and Flexible Platform for Autonomy Education and Research
						</h1>
						<p>
							In Duckietown, inhabitants (duckies) are transported via an autonomous mobility service
							(Duckiebots). Duckietown is designed to be inexpensive and modular, yet still enable many of
							the research and educational opportunities of a full-scale self-driving car platform.<br>
							<a href="https://duckietown-nctu.github.io/" target="_blank">Project Link</a> <br>
							<a href="summer_school.html" target="_blank">Duckietown Summer School in 2017</a> <br>
						</p>
					</div>
				</div>

				<!--20180306 samliu end-->

				<div class="fullcontent">
					<div class="teaser">
						<img src="research/bocelli-project-teaser.gif" width="210" height="100" border="0">
					</div>
					<div class="proj">
						<h1>Wearable Navigation System for the Blind and Visually Impaired</h1>
						<p>
							Our team develops wearable, miniaturized, energy-efficient, and soft robotic systems
							to provide key vision functionalities and navigation feedbacks for blind and visually
							impaired people.
							For safe mobility, we develop methods for independent awareness of obstacles, drop-offs,
							ascents, descents,
							and also important objects or landmarks.
							A haptic array and rapidly refreshable Braille device are used to deliver information about
							the surroundings.
							<a href="research/CPS_NickWang_poster_jpg.pdf" target="_blank">Poster</a>
							<a href="http://projects.csail.mit.edu/bocelli/" target="_blank">Website (2014)</a>
						</p>
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
						<img src="research/robotx_placard.gif" height="120" width="200" />
					</div>
					<div class="proj">
						<h1>RobotX Competition</h1>
						<p>
							The inaugural RobotX competition was held in Singapore in Oct. 2014.
							The purpose of the competition was to challenge teams to develop new strategies for tackling
							unique and
							important problems in marine robotics. Placard detection is one of the tasks, and our key
							design objectives were:
							1. Robustness to degradation caused by motion, scale and perspective transformation from
							different viewing positions,
							warp and occlusion caused by wind, and variants of color from light condition, 2. Speed and
							accuracy to support real-time decision-making.
							Ultimately the MIT/Olin team narrowly <strong>won first place</strong> in a competitive
							field.
							<a href="publications/fsr2015_robotx.pdf" target="_blank">Paper</a>
							<a href="http://newsoffice.mit.edu/2014/csail-meche-marine-experts-win-robotx-self-driving-boat-competition-1030"
								target="_blank">MIT News</a>
						</p>
					</div>
					<div class="fullcontent">
						<div class="teaser">
							<img src="research/multiple_observation_integration.gif" height="160" width="200" />
						</div>
						<div class="proj">
							<h1>Spatially Prioritized and Persistent Text Detection and Decoding</h1>
							<p>
								Our method uses simultaneous
								localization and mapping (SLAM) to extract planar “tiles” repre-
								senting scene surfaces.
								This paper’s contributions include: 1) spatiotemporal fusion
								of tile observations via SLAM, prior to inspection, thereby
								improving the quality of the input data; and 2) combination of
								multiple noisy text observations into a single higher-confidence
								estimate of environmental text.
								<a href="publications/Wang_cbdar2013.pdf" target="_blank">Paper</a>
							</p>
						</div>
					</div>

				</div>
				<h0>Visual Attention and Scene Perception</h0>
				<p></p>
				<div class="fullcontent">
					<div class="teaser">
						<p></p>
						<img src="research/jov_text_attraction.gif" width="130" height="130" border="0">
					</div>
					<div class="proj">
						<h1>The Attraction of Visual Attention to Texts in Real-World Scenes</h1>
						<h2><em>Where can you more likely find text?</em></h2>
						<p>
							The present study was aimed at investigating its underlying factors of human visual
							attention attracted by texts.
							The following main results were obtained: (a) Greater fixation probability and
							shorter minimum fixation distance of texts confirmed the higher attractiveness of texts; (b)
							the locations where texts are
							typically placed contribute partially to this effect; (c) specific visual features of texts,
							rather than typically salient features
							(e.g., color, orientation, and contrast), are the main attractors of attention; (d) the
							meaningfulness of texts does not add to
							their attentional capture; and (e) the attraction of attention depends to some extent on the
							observer’s familiarity with the
							writing system and language of a given text.
							<a href="publications/WangPomplun2012.pdf" target="_blank"> Paper</a>
						</p>
					</div>
				</div>
				<h0>Cognitive Models and Reading</h0>
				<p></p>
				<div class="fullcontent">
					<div class="teaser">
						<img src="research/brm.gif" width="210" height="200" border="0">
					</div>
					<div class="proj">
						<h1>Predicting Semantic Transparency Judgments</h1>
						<h2><em>Do you read “butter” when you read “butterfly”? Or “馬” for “馬虎”?</em></h2>
						The morphological constituents of English compounds and two-character Chinese compounds may
						differ in meaning from the whole word.
						The proposed models successfully predicted participants’ transparency ratings, and the
						prediction is influenced by the differences in raters’ morphological processing
						in the different writing systems. The dominance of lexical meaning, semantic transparency, and
						the average similarity between all pairs within a morphological family
						are provided, and practical applications for future studies are discussed.
						<a href="publications/Wang2014_brm.pdf" target="_blank">Paper</a>
						<a href="http://www.lsa.url.tw/modules/lsa/" target="_blank">Website</a>
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
						<p></p>
						<img src="research/chinese_svd.gif" width="210" height="160" border="0">
						<br><br><br>
					</div>
					<div class="proj">
						<h1>Degraded Character Recognition</h1>
						<h2><em>Can you read these sentence?</em></h2>
						<p>
							It is known that not all letters are of equal importance to the word recognition process,
							i.e., changing initial or exterior letters are more disruptchanging.
							Consistent results have been found for Chinese characters, and first-written strokes are
							more important for reading.
							This study compares the effects of high-level learnt stroke writting order and low-level
							features obtained via pixel-level singular value decompostion (SVD)
							to readers' degraded character recognition. Our results suggest that the most important
							segments determined by SVD, which has no information about stroke writing order, can
							identify first-written strokes. Furthermore, our results suggest that contour may be
							correlated with stroke writing order.
							<a href="publications/Wang2013-jrir.pdf" target="_blank"> Paper</a>
							<a href="publications/CICEM_Talk_NickWang.pdf" target="_blank">Slides</a>
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
					</div>
					<div class="proj">
					</div>
				</div>
				<div class="fullcontent">
					<div class="teaser">
					</div>
					<div class="proj">
					</div>

				</div>
				<div class="clearer">&nbsp;</div>

			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span>

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
