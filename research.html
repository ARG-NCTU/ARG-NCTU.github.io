<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
<meta name="description" content="description"/>
<meta name="keywords" content="keywords"/> 
<meta name="author" content="author"/> 
<link rel="stylesheet" type="text/css" href="default.css" media="screen"/>
<title>Nick Wang at NCTU</title>
</head>

<body>

<div class="outer-container">

<div class="inner-container">

	<div class="header">
		
		<div class="title">
			<span class="sitename"><a href="index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a href="index_ch.html">機器人與輔助科技實驗室</a></span>
		</div>

	</div>

	<div class="path">

		<a href="index.html">Home</a> 
		<a href="event.html">News</a> 
		<a href="people.html">People</a> 
		<a href="robots.html">Robots</a> 
		<a href="research.html">Research</a> 
		<a href="publications.html">Publications</a>
		<a href="courses.html">Education</a>
	</div>

	<div class="main">
		<h0>Robotics, Vision, Mobility, Manipulation</h0>
		<p></p>
		<!--20180306 samliu begin-->


		<div class="fullcontent">
			<div class="teaser">
				<img src="research/duckiepond.jpg" width="210" border="0">
			</div>
			<div class="proj">
				<h1>Duckiepond</h1>
				<p>
                    Duckiepond is an education and research development environment that includes software systems, educational materials, and of a fleet of autonomous surface vehicles Duckieboat. Duckieboats are designed to be easily reproducible with parts from a 3D printer and other commercially available parts, with flexible software that leverages several open source packages.
                </p>
					<a href="https://robotx-nctu.github.io/duckiepond" target="_blank">Project Link</a>
					<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
					<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
				</p>
			</div>
		</div>

		<div class="fullcontent">
			<div class="teaser">
				<img src="research/brandname.jpg" width="210" border="0">
			</div>
			<div class="proj">
				<h1>Brandname-based Manipulation</h1>
				<p>

We develop a pose-aware placement approach by spotting the semantic labels (e.g., brandnames) of objects in a cluttered tote and then carrying out a sequence of actions to place the objects on a shelf or on a conveyor with desired poses. Our major contributions include 1) providing an open benchmark dataset of objects and brandnames with multi-view segmentation for training and evaluations; 2) carrying out comprehensive evaluations for our brandname-based fully convolutional network (FCN) that can predict the affordance and grasp to achieve pose-aware placement, whose success rates decrease along with clutters; 3) showing that active manipulation with two cooperative manipulators and grippers can effectively handle the occlusion of brandnames.
					<a href="https://text-pick-n-place.github.io/TextPNP/" target="_blank">Project Link</a>
					<a href="TextPnP/index.html" target="_blank">Project Link Test!!!!!!!!!!</a>
					<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
					<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
				</p>
			</div>
		</div>


		<div class="fullcontent">
			<div class="teaser">
				<img src="research/robotic_guide_dog.jpg" width="210" border="0">
			</div>
			<div class="proj">
				<h1>Robotic Guide Dog Project</h1>
				<p>
					We develop the robotic guide dog which used a vision-based learning approach to autonomously follow man-made trails on various terrains of general pedestrian environments to enable independent navigation for people who are blind or visually impaired (BVI).<br>
					<a href="projects/robotic_guide_dog.html" target="_blank">Project Link</a>
					<!-- <a href="publications/ICRA18_0060_FI.pdf" target="_blank">Paper</a> -->
					<!-- <a href="https://vimeo.com/257645104" target="_blank">Video</a> -->
				</p>
			</div>
		</div>

		<div class="fullcontent">
			<div class="teaser">
				<img src="research/duckietown.jpg" width="210" border="0">
			</div>
			<div class="proj">
				<h1>Duckietown: an Open, Inexpensive and Flexible Platform for Autonomy Education and Research</h1>
				<p>
					In Duckietown, inhabitants (duckies) are transported via an autonomous mobility service (Duckiebots). Duckietown is designed to be inexpensive and modular, yet still enable many of the research and educational opportunities of a full-scale self-driving car platform.<br>
					<a href="https://duckietown-nctu.github.io/" target="_blank">Project Link</a> <br>
					<a href="summer_school.html" target="_blank">Duckietown Summer School in 2017</a> <br>
				</p>
			</div>
		</div>

		<!--20180306 samliu end-->

		<div class="fullcontent">
			<div class="teaser">
				<img src="research/bocelli-project-teaser.gif" width="210" height="100" border="0">
			</div>
			<div class="proj">
				<h1>Wearable Navigation System for the Blind and Visually Impaired</h1>
				<p>
					Our team develops wearable, miniaturized, energy-efficient, and soft robotic systems 
					to provide key vision functionalities and navigation feedbacks for blind and visually impaired people. 
					For safe mobility, we develop methods for independent awareness of obstacles, drop-offs, ascents, descents, 
					and also important objects or landmarks. 
					A haptic array and rapidly refreshable Braille device are used to deliver information about the surroundings. 
					<a href="research/CPS_NickWang_poster_jpg.pdf" target="_blank">Poster</a>
					<a href="http://projects.csail.mit.edu/bocelli/" target="_blank">Website (2014)</a>
				</p>
			</div>
		</div>
		<div class="fullcontent">
			<div class="teaser">
				<img src="research/robotx_placard.gif" height="120" width="200" />
			</div>
			<div class="proj">
				<h1>RobotX Competition</h1>
				<p>
				The inaugural RobotX competition was held in Singapore in Oct. 2014. 
				The purpose of the competition was to challenge teams to develop new strategies for tackling unique and
				important problems in marine robotics. Placard detection is one of the tasks, and our key design objectives were:
				1. Robustness to degradation caused by motion, scale and perspective transformation from different viewing positions, 
				warp and occlusion caused by wind, and variants of color from light condition, 2. Speed and accuracy to support real-time decision-making.
				Ultimately the MIT/Olin team narrowly <strong>won first place</strong> in a competitive field.
				<a href="publications/fsr2015_robotx.pdf" target="_blank">Paper</a>
				<a href="http://newsoffice.mit.edu/2014/csail-meche-marine-experts-win-robotx-self-driving-boat-competition-1030" target="_blank">MIT News</a>
				</p>
			</div>
		<div class="fullcontent">
			<div class="teaser">
				<img src="research/multiple_observation_integration.gif" height="160" width="200" />
			</div>
			<div class="proj">
				<h1>Spatially Prioritized and Persistent Text Detection and Decoding</h1>
				<p>
					Our method uses simultaneous
					localization and mapping (SLAM) to extract planar “tiles” repre-
					senting scene surfaces.
					This paper’s contributions include: 1) spatiotemporal fusion
					of tile observations via SLAM, prior to inspection, thereby
					improving the quality of the input data; and 2) combination of
					multiple noisy text observations into a single higher-confidence
					estimate of environmental text.
					<a href="publications/Wang_cbdar2013.pdf" target="_blank">Paper</a>
				</p>
			</div>
		</div>

		</div>
		<h0>Visual Attention and Scene Perception</h0>
		<p></p>
		<div class="fullcontent">
			<div class="teaser">
				<p></p>
				<img src="research/jov_text_attraction.gif" width="130" height="130" border="0">
			</div>
			<div class="proj">
				<h1>The Attraction of Visual Attention to Texts in Real-World Scenes</h1>
				<h2><em>Where can you more likely find text?</em></h2>
				<p>
					The present study was aimed at investigating its underlying factors of human visual attention attracted by texts. 
					The following main results were obtained: (a) Greater fixation probability and
					shorter minimum fixation distance of texts confirmed the higher attractiveness of texts; (b) the locations where texts are
					typically placed contribute partially to this effect; (c) specific visual features of texts, rather than typically salient features
					(e.g., color, orientation, and contrast), are the main attractors of attention; (d) the meaningfulness of texts does not add to
					their attentional capture; and (e) the attraction of attention depends to some extent on the observer’s familiarity with the
					writing system and language of a given text.
					<a href="publications/WangPomplun2012.pdf" target="_blank"> Paper</a>
				</p>
			</div>
		</div>
		<h0>Cognitive Models and Reading</h0>
		<p></p>
		<div class="fullcontent">
			<div class="teaser">
				<img src="research/brm.gif" width="210" height="200" border="0">
			</div>
			<div class="proj">
				<h1>Predicting Semantic Transparency Judgments</h1>
				<h2><em>Do you read “butter” when you read “butterfly”? Or “馬” for “馬虎”?</em></h2>
					The morphological constituents of English compounds and two-character Chinese compounds may differ in meaning from the whole word.
					The proposed models successfully predicted participants’ transparency ratings, and the prediction is influenced by the differences in raters’ morphological processing 
					in the different writing systems. The dominance of lexical meaning, semantic transparency, and the average similarity between all pairs within a morphological family 
					are provided, and practical applications for future studies are discussed.
					<a href="publications/Wang2014_brm.pdf" target="_blank">Paper</a>
					<a href="http://www.lsa.url.tw/modules/lsa/" target="_blank">Website</a>			
			</div>		
		</div>
		<div class="fullcontent">
			<div class="teaser">
				<p></p>
				<img src="research/chinese_svd.gif" width="210" height="160" border="0">
				<br><br><br>
			</div>
			<div class="proj">
				<h1>Degraded Character Recognition</h1>
				<h2><em>Can you read these sentence?</em></h2>
				<p>
					It is known that not all letters are of equal importance to the word recognition process, i.e., changing initial or exterior letters are more disruptchanging.
		 			Consistent results have been found for Chinese characters, and first-written strokes are more important for reading.
					This study compares the effects of high-level learnt stroke writting order and low-level features obtained via pixel-level singular value decompostion (SVD) 
					to readers' degraded character recognition. Our results suggest that the most important segments determined by SVD, which has no information about stroke writing order, 						can identify first-written strokes. Furthermore, our results suggest that contour may be correlated with stroke writing order. 
					<a href="publications/Wang2013-jrir.pdf" target="_blank"> Paper</a>
					<a href="publications/CICEM_Talk_NickWang.pdf" target="_blank">Slides</a>
			</div>
		</div>
		<div class="fullcontent">
			<div class="teaser">
			</div>
			<div class="proj">
			</div>	
		</div>
		<div class="fullcontent">
			<div class="teaser">
			</div>
			<div class="proj">
			</div>	
		
		</div>
		<div class="clearer">&nbsp;</div>

	</div>

	<div class="footer">

		<span class="left">
			&copy; H.C. Wang
		</span>

		<span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a href="http://arcsin.se/">Arcsin</a></span>

		<div class="clearer"></div>

	</div>

</div>

</div>

</body>

</html>
