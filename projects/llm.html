<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
						BT-Prompt: Hierarchical Behavior Tree Generation for High-level Task Planning using Large Language Models
					</h0>
					<h2>
						Ming-Fong Hsieh<sup>1</sup>, Wei-Li Liou<sup>1</sup>, Sun-Fu Chou<sup>1</sup>, Chang-Jin Wu<sup>1</sup>, Yu-Ting Ko<sup>1</sup>,
						Kuan-Ting Yu<sup>2</sup>, Hsueh-Cheng Wang<sup>*,1</sup>
					</h2>
					<h4>_________________________________________________________________</h4>

					<h4>
						<sup>1</sup>National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan.
					</h4>
					<h4>
						<sup>2</sup>XYZ Robotics
					</h4>

				<br><br>
				</div>

				<div class="wrapper-center">
					<button onclick="window.open('https://github.com/ARG-NCTU/bt_prompt')"
						class="button button-gray">Create Executable BT Script</button>
						<br>
					<button onclick="window.open('https://github.com/ARG-NCTU/bt_execution')"
						class="button button-gray">Execution</button>
					
					
					<!-- <button onclick="window.open('https://ieeexplore.ieee.org/document/')"
						class="button button-gray">paper</button> -->
					<!-- <button onclick="window.open('https://drive.google.com/drive/u/3/folders/1ylBJXGe7eM66R4D1h8D3BeKrQqKx7sqR')"
					class="button button-gray">Pre-trained Weight</button> -->

				</div>


				<h1>Abstract</h1>
				<p>
					Advancements in large language models (LLMs) have
					facilitated their incorporation into human-operated high-level
					robotic task planners. Research on task planning involving LLMs
					has focused on generating sequential plans; however, such plans
					might be insufficient for complex applications. We investigated the
					effects of prompt engineering on behavior tree (BT) generation by
					LLMs to enhance the performance of LLMs in few-shot example
					tasks and to improve task success rates. First, we developed BT-Prompt, an example task format that maximizes the few-shot
					learning performance of LLMs. Experiments indicated that LLM-generated BTs were more effective for task completion than other
					formats in few-shot scenarios. Second, we demonstrated that BT-Prompt can facilitate designing hierarchical task structures, i.e.,
					mid-level subtrees organizing a collection of low-level controllers.
					Experiments revealed that LLM-generated mid-level subtrees
					could achieve comparable success rates to those of hand-coded
					mid-level subtrees in complex tasks performed with various LLM
					backbones and under different temperature settings. LLMs with
					nonzero temperature were used to generate nondeterministic BTs
					for autonomously producing mid-level subtrees requiring minimal
					human guidance; these subtrees were tested through application
					in a series of challenging tasks. For these tasks, preexisting hand-coded mid-level subtrees were insufficient; however, integrating
					the LLM-generated subtrees with the preexisting hand-coded
					mid-level subtrees improved the task performance. Finally, the
					proposed system was successfully executed in virtual and real
					environments.
				</p>

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe title="vimeo-player" src="https://player.vimeo.com/video/926299982" width="640" height="360" frameborder="0" allowfullscreen></iframe>
				</div>

				<!-- <div class="wrapper-center">
					<img src="img/LLM-BT_teaser_v4.png" alt="teaser" height="350">
				</div> -->
				<!-- <br><br> -->
				
				<h1>Appendix A: BT-Prompt Generation and Translation Mechanism with Example for 3 Tasks</h1>

				<!-- BTs nodes Parsing Section -->
				<h2>Two steps of translating pseudo-code z into executable BT script y</h2>
				<section>
					<h3>BTs Nodes Parsing</h3>
					<p>This initial phase involves deconstructing the pseudo-code into a raw symbolic tree, identifying the distinct subtrees. <br>
						The steps include:</p>
					<ul>
					<li style="margin-left: 20px;"><strong>Parsing the Pseudo-Code Formatted BT into Tree Logic:</strong> This involves extracting the starting procedures of the tree from the provided responses and organizing the phases according to their layer.</li>
					<li style="margin-left: 20px;"><strong>Structure and Classification:</strong> Here, the phases are classified based on their hierarchical relationships and conditions (indicating success or failure).</li>
					<li style="margin-left: 20px;"><strong>Tree Construction:</strong> A tree structure is created based on the parsed information where node types (sequence, selector, parallel) are assigned based on the outcomes of the classification. These nodes are then interconnected to construct the overall tree architecture.</li>
					</ul>
				</section>
				
				<!-- Subtree Assembly Section -->
				<section>
					<h3>Subtree Assembly</h3>
					<p>In this subsequent step, the raw symbolic tree is further refined to become the final, executable symbolic BT. <br>
						This involves:</p>
					<ul>
					<li style="margin-left: 20px;"><strong>Tree Analysis:</strong> Traversing the entire tree to categorize the different elements of tree logic and identify all the subtree titles.</li>
					<li style="margin-left: 20px;"><strong>Subtree Replacement:</strong> This step involves substituting the placeholder subtree titles with actual subtrees, which contain specific conditions and actions pertinent to the behavior tree's functionality.</li>
					<li style="margin-left: 20px;"><strong>Final Output:</strong> The end product is a fully executable symbolic BT, equipped with all necessary conditions and actions for deployment.</li>
					</ul>
				</section>
				<!-- <div class="wrapper-center">
				<img src="img/LLM-BT_parser_symbolic_tree-V2.png" height="280" ></div> -->
				<h2>Three Tasks of GPT-3 BT Generation:</h2>
				The input prompt, pseudo-code formatted BT, and translated executable BT script in Experiment 1 of GPT-3 generation are shown in below links: <br>
				<a style="margin-left: 20px;" href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blob/main/examples/behavior_tree/aerial_robotics/visual_servoing/visual_servoing_using_text-davinci-003.md">Visual Servoing</a><br>
				<a style="margin-left: 20px;" href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blob/main/examples/behavior_tree/aerial_robotics/inspection/inspection_using_text-davinci-003.md">Inspection</a><br>
				<a style="margin-left: 20px;" href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blame/main/examples/behavior_tree/aerial_robotics/object_navigation/object_navigation_using_text-davinci-003.md">Object Navigation</a><br>
				<br>


				<h1>Appendix B: Low-level, Mid-level, and Final Tree of BT with Real-World Scenario</h1>
					In this appendix, you can see the relationship between low-level and mid-level subtrees, as well as their combination into final BTs.<br>
					Top: Hierarchical Subtrees of BTs, Low-level subtrees are primarily associated with basic position control, rotation control, and perception. Mid-level subtrees comprise multiple low-level subtrees that are either hand-coded (hc) or LLM-generated (llm).<br>
					Middle: Final tree, combination of low-level and mid-level subtrees into final tree.<br>
					Bottom: Different Explore plan, the trajectory of M<sup>hc</sup> explore pattern block and M<sup>llm</sup> explore subtree.
					<div class="wrapper-center">
					<img src="img/LLM-BT_mid-level-of-BT-v2.png" alt="teaser" height="600"></div>

				<h1>Appendix C: Executable BTs Threading and Multiprocessing Mechanisms with ROS</h1>

				<p>Parallel action in existing Pythonic code generation presents significant challenges due to the need for the LLM planner to generate multi-threading to handle concurrent actions. 
					To address this, we have developed a method that combines Behavior Trees (BT) with the Robot Operating System (ROS) to facilitate parallel actions more efficiently.
				<p>As illustrated in the upper part of the figure, the BT manages the logic of visual servoing, while the lower part shows the corresponding ROS node diagram. In this setup, 
					the condition nodes in the BT correspond to ROS topics (indicated by green circles), and the action nodes are managed by independent ROS nodes (denoted by red squares).
				<p>Specifically, in the domain of visual servoing, the controls for x, y, and z axes are implemented as three separate ROS nodes. 
					These nodes operate in different processes facilitated by the ROS infrastructure. Consequently, the parallel logic in the BT (symbolized by the parallel operator '||') can activate these action nodes simultaneously, enabling synchronized linear control along the x, y, and z axes for visual servoing.
				<p>This approach demonstrates that parallel actions can be effectively executed using our BT-prompt within the ROS framework, showcasing the potential for enhanced multi-threading in robotics applications.
				
				<div class="wrapper-center">
				<img src="img/LLM-BT_parallel_action.png" height="400" ></div>
				<br>

				<h1>Appendix D: New Tasks in Experiment 3 (RobotX Challenge 2022)</h1>
				<p><b>Detect and Dock (Dock.): </b> <br>
					In this task, there's a floating platform with three bays, each having a different color (red, green, or blue). 
					The AMS detects the color and docks in the bay with the matching color.
				<p><b>Scan the code (Scan.): </b> <br>	
					In this task, there's a three-sided light tower on a floating platform that displays a sequence of RGB lights. 
					The AMS observes the colors and their order and use this sequence for other tasks in later rounds.
				<p><b>Find and Fling (Shoot.): </b> <br>
					In this task, there's a floating platform with three panels, each having a colored square and two holes. 
					The AMS detects a designated color and shoots racquetballs through the panel's holes. 
					Each team gets four racquetballs for this.
				<p><b>Entrance and Exit Gates (Gate.): </b> <br>
					In this task, the AMS needs to go through gates marked by colored buoys and underwater beacons. 
					The AMS must detect the underwater beacon signals between these gates and enter through them before moving 
					on to other tasks. The task's complexity increases in each round and includes elements from other tasks.
					The beacons have different frequencies and activate one at a time during the task.There are three gates:
					<br>&nbsp &nbsp Gate 1 has a red and a white buoy.
					<br>&nbsp &nbsp Gate 2 has two white buoys.
					<br>&nbsp &nbsp Gate 3 has a white and a green buoy.

				<p><b>Wildlife Encounter (Wildlife.): </b> <br>
					Task involves three floating platforms that look like Australian marine animals: 
					a platypus, turtle, and crocodile. 
					The AMS detects and reacts to them using  Hyperspectral Imaging (HSI) camera . 
					Teams can use a UAV for help. After detection, the AMS circles the platypus clockwise, 
					the turtle counterclockwise, and the crocodile twice in any direction.
				<p><b>UAV Search and Report (Search.): </b> <br>
					In this land-based UAV task mimicking a search and rescue operation, 
					the UAV starts from one point, searches a marked field with orange markers, finds two objects there, 
					and lands at another designated point. Teams can use any search pattern within the field boundaries. 
					They report the objects they locate and their exact positions.
				<p><b>Follow the Path (Follow.): </b> <br>
					This task requires the Autonomous Maritime System (AMS) to follow a designated path of white buoys, 
					pass through six pairs of red and green buoys, and exit through another set of white buoys. 
					The AMS must steer clear of randomly placed obstacles, symbolized by round black buoys. 
					Teams have the option to employ a UAV for assistance in completing this task.
				<p><b>UAV Replenishment (Deliver.): </b> <br>
					This task involves using a UAV. The UAV takes off from a USV, finds a floating helipad, picks up a small colored tin, 
					delivers the tin to a circular target area on another floating helipad, and then returns to the USV.

				<h1>Appendix E: Analysis of Failure BT Generation - Two Types of Failure</h1>

				<p> <b>Type 1 - The inclusion of unnecessary subtrees or a lack of
					necessary subtrees can result in task failure.</b><br>
					Adding unnecessary subtrees or lacking essential
					subtrees results in task failure. When LLM generates the
					”inspection” task, it incorporates the subtree related to the
					red-boxed area into the task plan. However, as this subtree is
					designed for ”tracking object,” it requires a target object to
					fulfill the task’s completion requirement. Since the ”inspection”
					task does not involve detecting objects, the behavior tree may
					halt at this stage due to the absence of a target object, causing
					it to be unable to progress further.</p>

					<div class="wrapper-center">
					<img src="img/method_failure_1.png" alt="teaser" height="400"></div>

				<p><b>Type 2 - Misinterpretation of the logic expressed through
					example tasks can result in task failure.</b><br>
					Misinterpretation of the logic expressed through example
					tasks. When LLM generates the ”object navigation” task, it
					misinterprets the logic expression symbols within the red-
					boxed area (from fallback to sequence). Due to this incorrect
					interpretation of the logic expression symbols, the behavior
					tree is unable to transition to the tracking state even when it
					detects the target object.</p>
					
					<div class="wrapper-center">
					<img src="img/method_failure_2.png" alt="teaser" height="380"></div>
				<br>

				<h1>Appendix F: Real-World Inspection Trajectory with L and L + M<sup>hc</sup>  </h1>
				<div class="wrapper-center">
				<img src="img/LLM-BT_real_world_inspection_trajectory.png" height="400" ></div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
