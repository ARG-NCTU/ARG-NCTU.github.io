<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
						BT-Prompt: Hierarchical Behavior Tree Generation for High-level Task Planning using Large Language Models
					</h0>
					<h2>
						Ming-Fong Hsieh<sup>1</sup>, Wei-Li Liu<sup>1</sup>, Sun-Fu Chou<sup>1</sup>, Chang-Jin Wu<sup>1</sup>, Yu-Ting Ko<sup>1</sup>,
						Kuan-Ting Yu<sup>2</sup>, Hsueh-Cheng Wang<sup>*,1</sup>
					</h2>
					<h4>_________________________________________________________________</h4>

					<h4>
						<sup>1</sup>National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan.
					</h4>
					<h4>
						<sup>2</sup>XYZ Robotics
					</h4>

				<br><br>
				</div>

				<h1>Abstract</h1>
				<p>
					Recent advancement of Large Language Models (LLMs) have made it possible for LLMs to be incorporated in the development of 
					high-level task planners alongside human user. Prior works of LLMs emphaises on the generation of  sequential plans, thereby closing 
					the doors to execution of more complex applications. This research delves into how prompting engineering affect the generation of 
					Behavior Trees (BTs) using LLMs. In order to enhance the understanding of LLMs for few-shot example tasks and improve the success rate 
					of generated tasks. First, we introduced BT-Prompt in Experiment 1, a specially designed example task format which allows LLMs to 
					maximize their few-shot learning performance. The results obtained in Experiment 1 illustrate the effectiveness of LLM-generated BTs 
					compared to other formats in few-shot scenarios. Second, we aim to demonstrate our method's ability to design a hierarchy structure in 
					Experiment 2, in additionally this would highlights the pivotal role of mid-level subtrees in successfully executed complex tasks across 
					various LLM backbones and temperature settings. As intended by the research setup, for sub-optimal BTs, LLMs were used in attempts to 
					autonomously generate mid-level subtrees without human guidance. The result of these autonomously generated mid-level subtrees then 
					become a central element in Experiment 3, where we applied our methods to an series of challenging task defined by a robotics competition. 
					In this context, it became evident that the pre-existing hand-coded mid-level subtrees were insufficient, where as integrating the 
					LLM-generated mid-level subtrees into existing functions resulting in enhanced task execution for these intricate challenges. 
					Finally, this research collected illustrative examples of positive and negative prompting strategies of LLM-generated BTs, with 
					demonstrations of successful executions in virtual and real environments.
				</p>

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe title="vimeo-player" src="https://player.vimeo.com/video/874116829" width="640" height="360" frameborder="0" allowfullscreen></iframe>
				</div>

				<div class="wrapper-center">
					<img src="img/llm-teaser.png" alt="teaser" height="350">
				</div>
				<br><br>
				<h1>Appendix A : Two main failure types</h1>

				<p> <b>Type 1 - The inclusion of unnecessary subtrees or a lack of
					necessary subtrees can result in task failure.</b><br>
					Adding unnecessary subtrees or lacking essential
					subtrees results in task failure. When LLM generates the
					”inspection” task, it incorporates the subtree related to the
					red-boxed area into the task plan. However, as this subtree is
					designed for ”tracking object,” it requires a target object to
					fulfill the task’s completion requirement. Since the ”inspection”
					task does not involve detecting objects, the behavior tree may
					halt at this stage due to the absence of a target object, causing
					it to be unable to progress further.</p>

					<div class="wrapper-center">
					<img src="img/method_failure_1.png" alt="teaser" height="400"></div>

				<p><b>Type 2 - Misinterpretation of the logic expressed through
					example tasks can result in task failure.</b><br>
					Misinterpretation of the logic expressed through example
					tasks. When LLM generates the ”object navigation” task, it
					misinterprets the logic expression symbols within the red-
					boxed area (from fallback to sequence). Due to this incorrect
					interpretation of the logic expression symbols, the behavior
					tree is unable to transition to the tracking state even when it
					detects the target object.</p>
					
					<div class="wrapper-center">
					<img src="img/method_failure_2.png" alt="teaser" height="380"></div>
				<br>
				<h1>Appendix B : New Tasks in Experiment 3 (RobotX Challenge 2022)</h1>
				<p><b>Detect and Dock (Dock.): </b> <br>
					In this task, there's a floating platform with three bays, each having a different color (red, green, or blue). 
					The AMS detects the color and docks in the bay with the matching color.
				<p><b>Scan the code (Scan.): </b> <br>	
					In this task, there's a three-sided light tower on a floating platform that displays a sequence of RGB lights. 
					The AMS observes the colors and their order and use this sequence for other tasks in later rounds.
				<p><b>Find and Fling (Shoot.): </b> <br>
					In this task, there's a floating platform with three panels, each having a colored square and two holes. 
					The AMS detects a designated color and shoots racquetballs through the panel's holes. 
					Each team gets four racquetballs for this.
				<p><b>Entrance and Exit Gates (Gate.): </b> <br>
					In this task, the AMS needs to go through gates marked by colored buoys and underwater beacons. 
					The AMS must detect the underwater beacon signals between these gates and enter through them before moving 
					on to other tasks. The task's complexity increases in each round and includes elements from other tasks.
					The beacons have different frequencies and activate one at a time during the task.There are three gates:
					<br>&nbsp &nbsp Gate 1 has a red and a white buoy.
					<br>&nbsp &nbsp Gate 2 has two white buoys.
					<br>&nbsp &nbsp Gate 3 has a white and a green buoy.

				<p><b>Wildlife Encounter (Wildlife.): </b> <br>
					Task involves three floating platforms that look like Australian marine animals: 
					a platypus, turtle, and crocodile. 
					The AMS detects and reacts to them using  Hyperspectral Imaging (HSI) camera . 
					Teams can use a UAV for help. After detection, the AMS circles the platypus clockwise, 
					the turtle counterclockwise, and the crocodile twice in any direction.
				<p><b>UAV Search and Report (Search.): </b> <br>
					In this land-based UAV task mimicking a search and rescue operation, 
					the UAV starts from one point, searches a marked field with orange markers, finds two objects there, 
					and lands at another designated point. Teams can use any search pattern within the field boundaries. 
					They report the objects they locate and their exact positions.
				<p><b>Follow the Path (Follow.): </b> <br>
					This task requires the Autonomous Maritime System (AMS) to follow a designated path of white buoys, 
					pass through six pairs of red and green buoys, and exit through another set of white buoys. 
					The AMS must steer clear of randomly placed obstacles, symbolized by round black buoys. 
					Teams have the option to employ a UAV for assistance in completing this task.
				<p><b>UAV Replenishment (Deliver.): </b> <br>
					This task involves using a UAV. The UAV takes off from a USV, finds a floating helipad, picks up a small colored tin, 
					delivers the tin to a circular target area on another floating helipad, and then returns to the USV.
				
				<h1>Appendix C: BT-Prompt Generation for 3 Tasks</h1>
				The input prompt, pseudo-code formatted BT, and translated symbolic BT in Experiment 1 of GPT-3 generation are shown in below links:<br>
				There are 2 steps to translate the pseudo-code formatted BT to symbolic BT:</p>
				<p><strong>Post-processing:</strong> Split the pseudo-code formatted BT into raw symbolic tree with symbolic tree logic.<br>
					&nbsp &nbsp1.Parse the pseudo-code formatted BT into tree logic.<br>
					&nbsp &nbsp &nbsp &nbsp a.Extract procedures (start of the tree) from the response and group the phases by layer.<br>
					&nbsp &nbsp &nbsp &nbsp	b.Classify phases based on their hierarchical relationship and conditions (success or failure).<br>
					&nbsp &nbsp2.Create a tree structure based on the parsed data.<br>
					&nbsp &nbsp &nbsp &nbsp	a.Assign types to nodes (sequence, selector, parallel) based on the results of 1b.<br>
					&nbsp &nbsp &nbsp &nbsp	b.Connect nodes to form the tree.<br>
					&nbsp &nbsp3.Output raw symbolic tree with subtree titles.<br>
				
				<p><strong>Subtree assembly:</strong> After post-processing, we can obtain the raw symbolic tree, which is not the final executable symbolic tree with conditions and actions. Therefore, we apply subtree assembly to replace the subtree titles with the subtrees containing conditions and actions.<br>
					&nbsp &nbsp 1.Traverse the whole tree and classify tree logic and subtree titles.<br>
					&nbsp &nbsp 2.Replace subtree titles with the subtrees containing conditions and actions.<br>
					&nbsp &nbsp (e.g. object_in_view is a subtree with only condition node, explore_foward is a subtree with an action node and a codition node. )<br>
					&nbsp &nbsp 3.Output the final executable symbolic BT.<br>
	
				<div class="wrapper-center">
				<img src="img/LLM-BT_parser_symbolic_tree-V2.png" height="280" ></div>
				<p>3 tasks of GPT3 BT generation:</p>
				<a href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blob/main/examples/behavior_tree/aerial_robotics/visual_servoing/visual_servoing_using_text-davinci-003.md">Visual Servoing</a>
				<br><a href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blob/main/examples/behavior_tree/aerial_robotics/inspection/inspection_using_text-davinci-003.md">Inspection</a>
				<br><a href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blame/main/examples/behavior_tree/aerial_robotics/object_navigation/object_navigation_using_text-davinci-003.md">Object Navigation</a>
				<br>

				<h1>Appendix D: BT Parallel Action Mechanism with ROS</h1>
				<p>Parallel action in existing Pythonic code generation presents significant challenges due to the need for the LLM planner to generate multi-threading to handle concurrent actions. 
					To address this, we have developed a method that combines Behavior Trees (BT) with the Robot Operating System (ROS) to facilitate parallel actions more efficiently.
				<p>As illustrated in the upper part of the figure, the BT manages the logic of visual servoing, while the lower part shows the corresponding ROS node diagram. In this setup, 
					the condition nodes in the BT correspond to ROS topics (indicated by green circles), and the action nodes are managed by independent ROS nodes (denoted by red squares).
				<p>Specifically, in the domain of visual servoing, the controls for x, y, and z axes are implemented as three separate ROS nodes. 
					These nodes operate in different processes facilitated by the ROS infrastructure. Consequently, the parallel logic in the BT (symbolized by the parallel operator '||') can activate these action nodes simultaneously, enabling synchronized linear control along the x, y, and z axes for visual servoing.
				<p>This approach demonstrates that parallel actions can be effectively executed using our BT-prompt within the ROS framework, showcasing the potential for enhanced multi-threading in robotics applications.
				
				<div class="wrapper-center">
				<img src="img/LLM-BT_parallel_action.png" height="400" ></div>
	

				
			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
