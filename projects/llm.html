<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
						BT-Prompt: Hierarchical Behavior Tree Generation for High-level Task Planning using Large Language Models
					</h0>
					<h2>
						Ming-Fong Hsieh<sup>1</sup>, Wei-Li Liou<sup>1</sup>, Sun-Fu Chou<sup>1</sup>, Chang-Jin Wu<sup>1</sup>, Yu-Ting Ko<sup>1</sup>,
						Kuan-Ting Yu<sup>2</sup>, Hsueh-Cheng Wang<sup>*,1</sup>
					</h2>
					<h4>_________________________________________________________________</h4>

					<h4>
						<sup>1</sup>National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan.
					</h4>
					<h4>
						<sup>2</sup>XYZ Robotics
					</h4>

				<br><br>
				</div>

				<div class="wrapper-center">
					<button onclick="window.open('https://github.com/ARG-NCTU/bt_prompt')"
						class="button button-gray">Create Executable BT Script</button>
						<br>
					<button onclick="window.open('https://github.com/ARG-NCTU/bt_execution')"
						class="button button-gray">Execution</button>
					
					
					<!-- <button onclick="window.open('https://ieeexplore.ieee.org/document/')"
						class="button button-gray">paper</button> -->
					<!-- <button onclick="window.open('https://drive.google.com/drive/u/3/folders/1ylBJXGe7eM66R4D1h8D3BeKrQqKx7sqR')"
					class="button button-gray">Pre-trained Weight</button> -->

				</div>


				<h1>Abstract</h1>
				<p>
					Recent advancement of Large Language Models (LLMs) have made it possible for LLMs to be incorporated in the development of 
					high-level task planners alongside human user. Prior works of LLMs emphaises on the generation of  sequential plans, thereby closing 
					the doors to execution of more complex applications. This research delves into how prompting engineering affect the generation of 
					Behavior Trees (BTs) using LLMs. In order to enhance the understanding of LLMs for few-shot example tasks and improve the success rate 
					of generated tasks. First, we introduced BT-Prompt in Experiment 1, a specially designed example task format which allows LLMs to 
					maximize their few-shot learning performance. The results obtained in Experiment 1 illustrate the effectiveness of LLM-generated BTs 
					compared to other formats in few-shot scenarios. Second, we aim to demonstrate our method's ability to design a hierarchy structure in 
					Experiment 2, in additionally this would highlights the pivotal role of mid-level subtrees in successfully executed complex tasks across 
					various LLM backbones and temperature settings. As intended by the research setup, for sub-optimal BTs, LLMs were used in attempts to 
					autonomously generate mid-level subtrees without human guidance. The result of these autonomously generated mid-level subtrees then 
					become a central element in Experiment 3, where we applied our methods to an series of challenging task defined by a robotics competition. 
					In this context, it became evident that the pre-existing hand-coded mid-level subtrees were insufficient, where as integrating the 
					LLM-generated mid-level subtrees into existing functions resulting in enhanced task execution for these intricate challenges. 
					Finally, this research collected illustrative examples of positive and negative prompting strategies of LLM-generated BTs, with 
					demonstrations of successful executions in virtual and real environments.
				</p>

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe title="vimeo-player" src="https://player.vimeo.com/video/874116829" width="640" height="360" frameborder="0" allowfullscreen></iframe>
				</div>

				<!-- <div class="wrapper-center">
					<img src="img/LLM-BT_teaser_v4.png" alt="teaser" height="350">
				</div> -->
				<!-- <br><br> -->
				
				<h1>Appendix A : BT-Prompt Generation and Translation Mechanism with Example for 3 Tasks</h1>

				<!-- BTs nodes Parsing Section -->
				<h2>Two steps of translating pseudo-code z into executable BT script y</h2>
				<section>
					<h3>BTs nodes Parsing</h3>
					<p>This initial phase involves deconstructing the pseudo-code into a raw symbolic tree, identifying the distinct subtrees. <br>
						The steps include:</p>
					<ul>
					<li style="margin-left: 20px;"><strong>Parsing the Pseudo-Code Formatted BT into Tree Logic:</strong> This involves extracting the starting procedures of the tree from the provided responses and organizing the phases according to their layer.</li>
					<li style="margin-left: 20px;"><strong>Structure and Classification:</strong> Here, the phases are classified based on their hierarchical relationships and conditions (indicating success or failure).</li>
					<li style="margin-left: 20px;"><strong>Tree Construction:</strong> A tree structure is created based on the parsed information where node types (sequence, selector, parallel) are assigned based on the outcomes of the classification. These nodes are then interconnected to construct the overall tree architecture.</li>
					</ul>
				</section>
				
				<!-- Subtree Assembly Section -->
				<section>
					<h3>Subtree Assembly</h3>
					<p>In this subsequent step, the raw symbolic tree is further refined to become the final, executable symbolic BT. <br>
						This involves:</p>
					<ul>
					<li style="margin-left: 20px;"><strong>Tree Analysis:</strong> Traversing the entire tree to categorize the different elements of tree logic and identify all the subtree titles.</li>
					<li style="margin-left: 20px;"><strong>Subtree Replacement:</strong> This step involves substituting the placeholder subtree titles with actual subtrees, which contain specific conditions and actions pertinent to the behavior tree's functionality.</li>
					<li style="margin-left: 20px;"><strong>Final Output:</strong> The end product is a fully executable symbolic BT, equipped with all necessary conditions and actions for deployment.</li>
					</ul>
				</section>
				<!-- <div class="wrapper-center">
				<img src="img/LLM-BT_parser_symbolic_tree-V2.png" height="280" ></div> -->
				<h2>Three tasks of GPT3 BT generation:</h2>
				The input prompt, pseudo-code formatted BT, and translated symbolic BT in Experiment 1 of GPT-3 generation are shown in below links: <br>
				<a style="margin-left: 20px;" href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blob/main/examples/behavior_tree/aerial_robotics/visual_servoing/visual_servoing_using_text-davinci-003.md">Visual Servoing</a><br>
				<a style="margin-left: 20px;" href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blob/main/examples/behavior_tree/aerial_robotics/inspection/inspection_using_text-davinci-003.md">Inspection</a><br>
				<a style="margin-left: 20px;" href="https://github.com/ARG-NCTU/PromptCraft-Robotics/blame/main/examples/behavior_tree/aerial_robotics/object_navigation/object_navigation_using_text-davinci-003.md">Object Navigation</a><br>
				<br>


				<h1>Appendix B : Low-level, Mid-level, and Final Tree of BT with Real-World scenario</h1>
					In this appendix, you can see the relationship between low-level and mid-level subtrees, as well as their combination into final BTs.<br>
					Top: Hierarchical Subtrees of BTs, Low-level subtrees are primarily associated with basic position control, rotation control, and perception. Mid-level subtrees comprise multiple low-level subtrees that are either hand-coded (hc) or LLM-generated (llm).<br>
					Middle: Final tree, combination of low-level and mid-level subtrees into final tree.<br>
					Bottom: Different Explore plan, the trajectory of M<sup>hc</sup> explore pattern block and M<sup>llm</sup> explore subtree.
					<!-- In this appendix, you can see a list of low-level and mid-level subtrees, as well as their combination into final BTs.<br>
                    Top-Left: Low-level subtrees are primarily associated with basic position control, rotation control, and perception. <br>
					Top-Right: Mid-level subtrees comprise multiple low-level subtrees that are either hand-coded (hc) or LLM-generated (llm).<br>
                    We showcase an example of the final BT with (bottom-left) or without (bottom-right) hand-coded mid-level subtrees. <br>
					Bottom-right: Without providing M<sup>hc</sup>, LLM generates the functionally equivalent M<sup>llm</sup> to complete the task.<br> -->
					<div class="wrapper-center">
					<img src="img/LLM-BT_mid-level-of-BT-v2.png" alt="teaser" height="600"></div>

				<h1>Appendix C : Executable BTs Threading and Multiprocessing Mechanisms with ROS</h1>

				<p>Parallel action in existing Pythonic code generation presents significant challenges due to the need for the LLM planner to generate multi-threading to handle concurrent actions. 
					To address this, we have developed a method that combines Behavior Trees (BT) with the Robot Operating System (ROS) to facilitate parallel actions more efficiently.
				<p>As illustrated in the upper part of the figure, the BT manages the logic of visual servoing, while the lower part shows the corresponding ROS node diagram. In this setup, 
					the condition nodes in the BT correspond to ROS topics (indicated by green circles), and the action nodes are managed by independent ROS nodes (denoted by red squares).
				<p>Specifically, in the domain of visual servoing, the controls for x, y, and z axes are implemented as three separate ROS nodes. 
					These nodes operate in different processes facilitated by the ROS infrastructure. Consequently, the parallel logic in the BT (symbolized by the parallel operator '||') can activate these action nodes simultaneously, enabling synchronized linear control along the x, y, and z axes for visual servoing.
				<p>This approach demonstrates that parallel actions can be effectively executed using our BT-prompt within the ROS framework, showcasing the potential for enhanced multi-threading in robotics applications.
				
				<div class="wrapper-center">
				<img src="img/LLM-BT_parallel_action.png" height="400" ></div>
				<br>

				<h1>Appendix D : New Tasks in Experiment 3 (RobotX Challenge 2022)</h1>
				<p><b>Detect and Dock (Dock.): </b> <br>
					In this task, there's a floating platform with three bays, each having a different color (red, green, or blue). 
					The AMS detects the color and docks in the bay with the matching color.
				<p><b>Scan the code (Scan.): </b> <br>	
					In this task, there's a three-sided light tower on a floating platform that displays a sequence of RGB lights. 
					The AMS observes the colors and their order and use this sequence for other tasks in later rounds.
				<p><b>Find and Fling (Shoot.): </b> <br>
					In this task, there's a floating platform with three panels, each having a colored square and two holes. 
					The AMS detects a designated color and shoots racquetballs through the panel's holes. 
					Each team gets four racquetballs for this.
				<p><b>Entrance and Exit Gates (Gate.): </b> <br>
					In this task, the AMS needs to go through gates marked by colored buoys and underwater beacons. 
					The AMS must detect the underwater beacon signals between these gates and enter through them before moving 
					on to other tasks. The task's complexity increases in each round and includes elements from other tasks.
					The beacons have different frequencies and activate one at a time during the task.There are three gates:
					<br>&nbsp &nbsp Gate 1 has a red and a white buoy.
					<br>&nbsp &nbsp Gate 2 has two white buoys.
					<br>&nbsp &nbsp Gate 3 has a white and a green buoy.

				<p><b>Wildlife Encounter (Wildlife.): </b> <br>
					Task involves three floating platforms that look like Australian marine animals: 
					a platypus, turtle, and crocodile. 
					The AMS detects and reacts to them using  Hyperspectral Imaging (HSI) camera . 
					Teams can use a UAV for help. After detection, the AMS circles the platypus clockwise, 
					the turtle counterclockwise, and the crocodile twice in any direction.
				<p><b>UAV Search and Report (Search.): </b> <br>
					In this land-based UAV task mimicking a search and rescue operation, 
					the UAV starts from one point, searches a marked field with orange markers, finds two objects there, 
					and lands at another designated point. Teams can use any search pattern within the field boundaries. 
					They report the objects they locate and their exact positions.
				<p><b>Follow the Path (Follow.): </b> <br>
					This task requires the Autonomous Maritime System (AMS) to follow a designated path of white buoys, 
					pass through six pairs of red and green buoys, and exit through another set of white buoys. 
					The AMS must steer clear of randomly placed obstacles, symbolized by round black buoys. 
					Teams have the option to employ a UAV for assistance in completing this task.
				<p><b>UAV Replenishment (Deliver.): </b> <br>
					This task involves using a UAV. The UAV takes off from a USV, finds a floating helipad, picks up a small colored tin, 
					delivers the tin to a circular target area on another floating helipad, and then returns to the USV.

				<h1>Appendix E : Analysis of Failure BT Generation - Two Types of Failure</h1>

				<p> <b>Type 1 - The inclusion of unnecessary subtrees or a lack of
					necessary subtrees can result in task failure.</b><br>
					Adding unnecessary subtrees or lacking essential
					subtrees results in task failure. When LLM generates the
					”inspection” task, it incorporates the subtree related to the
					red-boxed area into the task plan. However, as this subtree is
					designed for ”tracking object,” it requires a target object to
					fulfill the task’s completion requirement. Since the ”inspection”
					task does not involve detecting objects, the behavior tree may
					halt at this stage due to the absence of a target object, causing
					it to be unable to progress further.</p>

					<div class="wrapper-center">
					<img src="img/method_failure_1.png" alt="teaser" height="400"></div>

				<p><b>Type 2 - Misinterpretation of the logic expressed through
					example tasks can result in task failure.</b><br>
					Misinterpretation of the logic expressed through example
					tasks. When LLM generates the ”object navigation” task, it
					misinterprets the logic expression symbols within the red-
					boxed area (from fallback to sequence). Due to this incorrect
					interpretation of the logic expression symbols, the behavior
					tree is unable to transition to the tracking state even when it
					detects the target object.</p>
					
					<div class="wrapper-center">
					<img src="img/method_failure_2.png" alt="teaser" height="380"></div>
				<br>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
