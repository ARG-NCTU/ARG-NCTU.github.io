<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
						Fed-HANet: Federated Visual Grasping Learning for Human Robot Handovers
					</h0>
					<h2>
						Ching-I Huang, Yu-Yen Huang, Jie-Xin Liu, Yu-Ting Ko, Hsueh-Cheng Wang<sup>*</sup>
					</h2>
				<br><br>
				</div>

				<div class="wrapper-center">
					<button onclick="window.open('https://github.com/ARG-NCTU/handover_grasping')"
						class="button button-gray">Interface</button>
					<button onclick="window.open('https://github.com/ARG-NCTU/handover-system')"
						class="button button-gray">System</button>
					<!-- <button onclick="window.open('https://drive.google.com/drive/u/3/folders/1ylBJXGe7eM66R4D1h8D3BeKrQqKx7sqR')"
					class="button button-gray">Pre-trained Weight</button> -->

				</div>

				<h1>Abstract</h1>
				<p>
					Human-robot handover is a key capability of service robots, such as those performing routine logistical tasks
					for healthcare workers. The handover problem is generally addressed by respectively detecting the hand and object prior
					to implementing a grasping planner. Recent algorithms have achieved tremendous advances in object-agnostic end-to-end
					planar grasping with up to six degrees of freedom (DoF); however, compiling the requisite datasets is infeasible in many
					situations and the use of camera feeds would be considered invasive in other situations. In this study, we proposed Fed-
					HANet, trained with the framework of federated learning, to achieve a privacy-preserving end-to-end control system for 6-
					DoF visual grasping of unseen objects. In experiments involving novel objects, the federated learning framework permitted a
					high degree of selectivity at local picking stations with accuracy close to that of systems based on centralized training. The
					practicality of Fed-HANet in robotic systems was assessed in a user study involving 15 participants. The dataset for
					training and all pretrained models are available at 
					<a
						href='https://arg-nctu.github.io/projects/fed-hanet.html. '>https://arg-nctu.github.io/projects/fed-hanet.html</a>.
				</p>

				<div class="wrapper-center">
					<img src="img/handover_smart_health.png" alt="teaser" height="400">
				</div>
				

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<!--<iframe src="https://www.youtube.com/embed/tR1NMvnTvMU" title="HANet"  width="640" height="360" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
					<iframe title="vimeo-player" src="https://player.vimeo.com/video/759768324?h=19f001e006" width="640" height="360" frameborder="0" allowfullscreen></iframe>
				</div>
				<br><br><br>
				<p>The pipeline for grasping </p>
				<img src="img/handover_pipeline.png" width="750">
				<br><br><br>

				<h1>Supplementary Materials</h1>
				<p> Failure case of HANet(first row) and HANet-depth(second row), with HA-Upright dataset.  *Use RTX 3060Ti</p>
				<p> The number of failure case of HANet-depth is 35, while the number of HANet is only 19 in total 200 testing images. Failure rates of the two algorithms are 6.25% and 9.5% respectively.</p>
				<div class="container" align="middle">
					<img src="img/handover_failure1.png" width="670">
				</div>
				<p> Failure case of HANet(first row) and HANet-depth(second row), with HA-Rotated dataset.  *Use RTX 3060Ti</p>
				<p> The number of failure case of HANet-depth is 89, while the number of HANet is only 15 in total 240 testing images. Failure rates of the two algorithms are 37.2% and 17.5% respectively.</p>
				<div class="container" align="middle">
					<img src="img/handover_failure2.png" width="760">
				</div>
				<br><br><br>
				<!--
				<h1>THE PROPOSED HANDOVER NETWORK (HANET)</h1>
				<p> Our key designs differed from existing
					methods were to produce a 6-DOF affordance of both
					orientations through view synthesis at the  multiple views. 
					The handover network (HANet) was designed to address the limitations of existing
					method ConvNet [32] of end-to-end affordance prediction
					of object grasping (top and down). Our HANet used a
					similar model architecture to ConvNet, but with the 4-fold
					modifications: 
				</p>
				<p>(1) output layer for classification of orientation, </p>
				<p>(2) estimation for grasping atemp with different directions, </p>
				<p>(3) multiple views to achieve 6-DOF grasping and</p>
				<p>(4) training datasets for enabling both centralized and federated
					learning training paradigms for privaciy concern. </p>
					
				
				<div class="wrapper-center">
					<p> The proposed HANet </p>
					<img src="img/handover_concept.png" width="600">
					<br><br><br>
					<p>Comparisons of handover and visual grasping approaches</p> 
					<img src="img/handover_approaches.png" width="500">
					<br><br><br><br><br><br>
					<p>The pipeline for grasping </p>
					<img src="img/handover_pipeline.png" width="750">
					<br><br><br>
					<p> Labeling for classification of orientation</p>
					<img src="img/handover_labeling.png" width="750">
					<br><br><br>
					<img src="img/handover_datasets.png" width="500">
					<br><br><br>
					<p> Object sets of HANet for training, testing and user study</p>
					<img src="img/handover_objectsets.png" width="500">
					<br><br><br>
					

				</div>
				<br><br><br>-->
<!-- 
				<h1>HANet Estimation</h1>
				<p> We used a set of indicators to
					evaluate the performance of the system and analyze the performance by
					differenct testing datasets in Table II. We visualize our 2-finger parallel gripper 
					as a line segment in each image in each testing dataset. 
					The center points were the top-1 accuracy of grasp prediction; both ends of the line
					segments represented the positions of the two fingers at the
					open state of the gripper (8cm). 
				</p>
				
				<div class="wrapper-center">
					<p> Visualizations of grasping prediction on the 4 testing datasets. </p>
					<img src="img/handover_estimation_visual.png" height="400">
					<br><br><br>
					<img src="img/handover_estimation.png" height="250">
					<br><br><br>
				</div>
				<br><br><br>

				
				<h1>Fed-HANet Evaluations</h1>
				<div class="wrapper-center">
					<p>We evaluated the training paradigms with the different
						settings and baselines. The accuracy was
						calculated using the testing set HA-Rotated.</p>
					<img src="img/handover_federated_learning.png" width="500">
					<br><br><br>
					<p>There are Grasp Type  I and Grasp Type II preferences for picking objects. Grasp Type I is picking the middle of objects, and Grasp Type II is picking the top of objects, using different perspectives to predict objects, and finally selecting the grasping perspective preferred by humans.</p>
					<img src="img/medical-socially_experiment.PNG" width="400">
				</div>
				<br><br><br>

				<h1>User study</h1>
				<div class="wrapper-center">
					<p>We evaluated the robotic handover system together with the proposed HANet. 
						15 participants (ages 20-30) were recruited, and all of them did not have any prior experience with our system. 
						The task was designed to allow a participant to hold an object in arbitrary directions and orientation . 
					</p>
					<img src="img/handover_userstudy.png" width="600">
					<br><br><br>
					<img src="img/handover_questionnaire.png" width="600">
					<br><br><br>

				</div>
				<br><br><br>-->

<!-- 
				<h1 id="datasets">Datasets</h1>

				<h2>Handover Dataset</h2>
				<p>(1) water cup, (2) small medicine cup, (3) medicine bottle, (4) medicine box, (5) SPAM, (6) banana, (7) lemon, (8) strawberry, (9) peach, (10) pear, (11) plum, (12) mustard, (13) sugar. Fruit objects were plastic.</p>
				<div class="wrapper-center">
					<img src="img/medical-dataset-1.png" height="400">
				</div>
				<br><br><br>

				<p>Comparison table of three datasets of HERoS. HERoS-Af: subset for predicting affordance; HERoS-Ch: subset for learning grasp choice; HERoS-Tr: subset for studying trajectories of human demonstrations.</p>
				<div class="wrapper-center">
					<img src="img/handover-dataset.png" height="130">
				</div>
				<br><br><br>

				<h2>HERoS-Affordance-Prediction</h2>
				<a
				href="https://drive.google.com/file/d/13vRFPhruy37sk3Tzo4Z6dyw--TFWSIuH/view?usp=sharing">click here to download HERoS-Af dataset.</a>

				<h2>HERoS-Grasp-Choice</h2>
				<a
				href="https://drive.google.com/file/d/11msoIZqSIgLuL9sEvXzXVHcpn5zOBrN9/view?usp=sharing">click here to download HERoS-Ch dataset.</a>

				<h2>HERoS-Trajectories</h2>
				<a
				href="https://drive.google.com/file/d/19wq5iy8EB3XBzr0wJyQG-C4-LRB0Lrzc/view?usp=sharing">click here to download HERoS-Tr dataset.</a>

				<h2>Dataset Description</h2>
				<a
				href="https://docs.google.com/document/d/1EBUff3kaGFE9QXFRdUeilCbgAC3Sc67t/edit?usp=sharing&ouid=114003070294845923127&rtpof=true&sd=true">click here to download dataset description.</a> -->








				<div class="clearer">&nbsp;</div>

			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<!-- <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span> -->

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
