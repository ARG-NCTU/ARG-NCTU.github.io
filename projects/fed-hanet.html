<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
						Fed-HANet: Federated Visual Grasping Learning for Human Robot Handovers
					</h0>
					<h2>
						Ching-I Huang, Yu-Yen Huang, Jie-Xin Liu, Yu-Ting Ko, Hsueh-Cheng Wang<sup>*</sup>
					</h2>
				<br><br>
				</div>

				<div class="wrapper-center">
					<button onclick="window.open('https://github.com/ARG-NCTU/handover_grasping')"
						class="button button-gray">Interface</button>
					<button onclick="window.open('https://github.com/ARG-NCTU/handover-system')"
						class="button button-gray">System</button>
					<!-- <button onclick="window.open('https://drive.google.com/drive/u/3/folders/1ylBJXGe7eM66R4D1h8D3BeKrQqKx7sqR')"
					class="button button-gray">Pre-trained Weight</button> -->

				</div>

				<h1>Abstract</h1>
				<p>
					Human-to-robot (H2R) handover is a key capability of service robots, but few of recent works have addressed the challenge at the diverse and unknown objects.  
					Although end-to-end grasping directly has made tremendous progress at agnostic object in logistic tasks, most grasping algorithms are limited to top-down movements.
					Due to the variable preference on holding and delivering by the operators, it has not been mapped to H2R. 
					We present an approach for end-to-end reactive 6DoF grasp choice by inferring dense pixel-wise probability maps of the affordances with multiple views. 
					Besides of closed-loop real-time motion planning, our system not only hand over the object with suitable grasping angles, but also allows the robot arm to grasp the object via the narrowest side of the objects. 
					Hence, our system can adapt to user' preference and a wider range of object categories with the same gripper. 
					In addition, our approach can also apply to federated learning framework to keep our personal data private. 
					We compare our results of evaluation to 3 state-of-the-art approaches and verify ours on 4 novel benchmark sets of the objects. 
					Furthermore, we demonstrate the effectiveness through 2 user study with 12 participants respectively. 
					The dataset for training and all pretrained models are available at  
					<a
						href='https://arg-nctu.github.io/projects/socially-aware-handover.html'>https://arg-nctu.github.io/projects/socially-aware-handover.html</a>.
				</p>

				<div class="wrapper-center">
					<img src="img/handover_smart_health.png" alt="teaser" height="400">
				</div>
				

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe src="https://www.youtube.com/embed/tR1NMvnTvMU" title="HANet"  width="640" height="360" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				</div>
				<br><br><br>
				<p>The pipeline for grasping </p>
				<img src="img/handover_pipeline.png" width="750">
				<br><br><br>
				<!--
				<h1>THE PROPOSED HANDOVER NETWORK (HANET)</h1>
				<p> Our key designs differed from existing
					methods were to produce a 6-DOF affordance of both
					orientations through view synthesis at the  multiple views. 
					The handover network (HANet) was designed to address the limitations of existing
					method ConvNet [32] of end-to-end affordance prediction
					of object grasping (top and down). Our HANet used a
					similar model architecture to ConvNet, but with the 4-fold
					modifications: 
				</p>
				<p>(1) output layer for classification of orientation, </p>
				<p>(2) estimation for grasping atemp with different directions, </p>
				<p>(3) multiple views to achieve 6-DOF grasping and</p>
				<p>(4) training datasets for enabling both centralized and federated
					learning training paradigms for privaciy concern. </p>
					
				
				<div class="wrapper-center">
					<p> The proposed HANet </p>
					<img src="img/handover_concept.png" width="600">
					<br><br><br>
					<p>Comparisons of handover and visual grasping approaches</p> 
					<img src="img/handover_approaches.png" width="500">
					<br><br><br><br><br><br>
					<p>The pipeline for grasping </p>
					<img src="img/handover_pipeline.png" width="750">
					<br><br><br>
					<p> Labeling for classification of orientation</p>
					<img src="img/handover_labeling.png" width="750">
					<br><br><br>
					<img src="img/handover_datasets.png" width="500">
					<br><br><br>
					<p> Object sets of HANet for training, testing and user study</p>
					<img src="img/handover_objectsets.png" width="500">
					<br><br><br>
					

				</div>
				<br><br><br>-->
<!-- 
				<h1>HANet Estimation</h1>
				<p> We used a set of indicators to
					evaluate the performance of the system and analyze the performance by
					differenct testing datasets in Table II. We visualize our 2-finger parallel gripper 
					as a line segment in each image in each testing dataset. 
					The center points were the top-1 accuracy of grasp prediction; both ends of the line
					segments represented the positions of the two fingers at the
					open state of the gripper (8cm). 
				</p>
				
				<div class="wrapper-center">
					<p> Visualizations of grasping prediction on the 4 testing datasets. </p>
					<img src="img/handover_estimation_visual.png" height="400">
					<br><br><br>
					<img src="img/handover_estimation.png" height="250">
					<br><br><br>
				</div>
				<br><br><br>

				
				<h1>Fed-HANet Evaluations</h1>
				<div class="wrapper-center">
					<p>We evaluated the training paradigms with the different
						settings and baselines. The accuracy was
						calculated using the testing set HA-Rotated.</p>
					<img src="img/handover_federated_learning.png" width="500">
					<br><br><br>
					<p>There are Grasp Type  I and Grasp Type II preferences for picking objects. Grasp Type I is picking the middle of objects, and Grasp Type II is picking the top of objects, using different perspectives to predict objects, and finally selecting the grasping perspective preferred by humans.</p>
					<img src="img/medical-socially_experiment.PNG" width="400">
				</div>
				<br><br><br>

				<h1>User study</h1>
				<div class="wrapper-center">
					<p>We evaluated the robotic handover system together with the proposed HANet. 
						15 participants (ages 20-30) were recruited, and all of them did not have any prior experience with our system. 
						The task was designed to allow a participant to hold an object in arbitrary directions and orientation . 
					</p>
					<img src="img/handover_userstudy.png" width="600">
					<br><br><br>
					<img src="img/handover_questionnaire.png" width="600">
					<br><br><br>

				</div>
				<br><br><br>-->

<!-- 
				<h1 id="datasets">Datasets</h1>

				<h2>Handover Dataset</h2>
				<p>(1) water cup, (2) small medicine cup, (3) medicine bottle, (4) medicine box, (5) SPAM, (6) banana, (7) lemon, (8) strawberry, (9) peach, (10) pear, (11) plum, (12) mustard, (13) sugar. Fruit objects were plastic.</p>
				<div class="wrapper-center">
					<img src="img/medical-dataset-1.png" height="400">
				</div>
				<br><br><br>

				<p>Comparison table of three datasets of HERoS. HERoS-Af: subset for predicting affordance; HERoS-Ch: subset for learning grasp choice; HERoS-Tr: subset for studying trajectories of human demonstrations.</p>
				<div class="wrapper-center">
					<img src="img/handover-dataset.png" height="130">
				</div>
				<br><br><br>

				<h2>HERoS-Affordance-Prediction</h2>
				<a
				href="https://drive.google.com/file/d/13vRFPhruy37sk3Tzo4Z6dyw--TFWSIuH/view?usp=sharing">click here to download HERoS-Af dataset.</a>

				<h2>HERoS-Grasp-Choice</h2>
				<a
				href="https://drive.google.com/file/d/11msoIZqSIgLuL9sEvXzXVHcpn5zOBrN9/view?usp=sharing">click here to download HERoS-Ch dataset.</a>

				<h2>HERoS-Trajectories</h2>
				<a
				href="https://drive.google.com/file/d/19wq5iy8EB3XBzr0wJyQG-C4-LRB0Lrzc/view?usp=sharing">click here to download HERoS-Tr dataset.</a>

				<h2>Dataset Description</h2>
				<a
				href="https://docs.google.com/document/d/1EBUff3kaGFE9QXFRdUeilCbgAC3Sc67t/edit?usp=sharing&ouid=114003070294845923127&rtpof=true&sd=true">click here to download dataset description.</a> -->








				<div class="clearer">&nbsp;</div>

			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<!-- <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span> -->

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
