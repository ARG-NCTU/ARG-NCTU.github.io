<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
						Cross-Modal Contrastive Learning of Representations for Navigation using Lightweight,
						Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions
					</h0>
				</div>
				<br><br>

				<div class="wrapper-center">
					<button onclick="window.open('http://github.com/huangjuite/radar-navigation')"
						class="button button-gray">Github</button>
					<button onclick="location.href='#datasets'" class="button button-gray">Dataset</button>
					<button onclick="window.open('https://ieeexplore.ieee.org/document/9362209')"
						class="button button-gray">Paper</button>
				</div>

				<h1>Abstract</h1>
				<p>
					Deep reinforcement learning (RL), where the agent learns from mistakes, has been successfully
					applied to a variety of tasks. With the aim of learning collision-free policies for unmanned
					vehicles, deep RL has been used for training with various types of data, such as colored images,
					depth images, and LiDAR point clouds, without the use of classic map--localize--plan approaches.
					However, existing methods are limited by their reliance on cameras and LiDAR devices, which have
					degraded sensing under adverse environmental conditions (e.g., smoky environments). In response, we
					propose the use of single-chip millimeter-wave (mmWave) radar, which is lightweight and inexpensive,
					for learning-based autonomous navigation. However, because mmWave radar signals are often noisy and
					sparse, we propose a cross-modal contrastive learning for representation (CM-CLR) method that
					maximizes the agreement between mmWave radar data and LiDAR data in the training stage. We evaluated
					our method in real-world robot compared with 1) a method with two separate networks using
					cross-modal generative reconstruction and an RL policy and 2) a baseline RL policy without
					cross-modal representation. Our proposed end-to-end deep RL policy with contrastive learning
					successfully navigated the robot through smoke-filled maze environments and achieved better
					performance compared with generative reconstruction methods, in which noisy artifact walls or
					obstacles were produced. All pretrained models and hardware settings are open access for reproducing
					this study and can be obtained at
					<a
						href='https://arg-nctu.github.io/projects/deeprl-mmWave.html'>https://arg-nctu.github.io/projects/deeprl-mmWave.html</a>.
				</p>

				<div class="wrapper-center">
					<img src="img/teaser-mmrl.png" alt="hdf5-example" height="250">
				</div>




				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe src="https://player.vimeo.com/video/468839536" width="640" height="360" frameborder="0"
						allow="autoplay; fullscreen" allowfullscreen></iframe>
				</div>
				<br><br><br>

				<h1>Generative reconstruction</h1>
				<p>range were ploted on cartesian coordinate system</p>
				<div class="wrapper-center">
					<img src="img/demo.png" height="400">
				</div>
				<br><br><br>

				<h1>Contrastive learning</h1>

				<div class="wrapper-center">
					<p>Contrastive learning structure</p>
					<img src="img/contrastive-v3.png" width="400">
					<br><br><br>
					<p>The network is trained for 20 episodes with
						our dataset</p>
					<img src="img/mm_loss.png" width="400">
				</div>
				<br><br><br>

				<h1>Deep reinforcement learning</h1>

				<div class="wrapper-center">
					<p>model structure</p>
					<img src="img/model.jpeg" width="550">
					<br><br><br>
					<p>Episode return reward with standard deviation of agent training using RDPG and DDPG.</p>
					<img src="img/reward.png" width="550">
					<br><br><br>
					<p>Performance evaluation of agents trained using different RL algorithms. We considered a safe
						navigation without collision a trial. A trial was terminated when the robot collided walls or
						obstacles, reached to a maximum distance of 1.2 kilometers, or manually stopped by human
						intervention. The travelled distance was estimated by the wheel encoders provided by the
						Clearpath Husky robots.</p>
					<img src="img/rl_eval.png" width="400">
				</div>
				<br><br><br>


				<h1 id="datasets">Datasets</h1>
				<h2>
					<ul style="margin-left: 40px">
						<li style="list-style:circle; font-size:18px">rosbag</li>
						<p>The original recorded format. All data are recorded in realtime. If user are familiar with
							ROS. There is <a href="http://wiki.ros.org/rosbag">rosbga API</a> to extract data from bags
							or using <a href="http://wiki.ros.org/rosbag/Commandline">commandline</a> to play data in
							realtime. <a href="http://wiki.ros.org/rviz">Rviz</a> can help the visualization.
							<br><br>
							<a
								href="https://drive.google.com/drive/folders/1KSccVqYDShI_Acfk8F4k_N76aVVPdN4o?usp=sharing">click
								here to download all rosbags.</a>
						</p>

						<li style="list-style:circle; font-size:18px">hdf5</li>
						<p>Data in rosbags has been transfer to numpy arrays, stored in hdf5 format and
							organized to 5Hz framerate. Including following data:
							<li style="list-style:circle;margin-left: 40px">LiDAR <br> 3D point cloud <br>1D range
							</li>
							<li style="list-style:circle;margin-left: 40px">mmWave radar <br> 3D point cloud <br>1D
								range</li>
							<li style="list-style:circle;margin-left: 40px">human demonstration action</li>
							<li style="list-style:circle;margin-left: 40px">wheel oddmetry</li>
							<br>
							User can use <a href="https://github.com/ganymede42/h5pyViewer">h5pyViewer</a> to check the
							data.
							<br>
						<div class="wrapper-center">
							<img src="img/hdf5-example.png" alt="hdf5-example" height="300">
						</div>

						<br><br>
						<a href="https://drive.google.com/drive/u/1/folders/10W1_F36Ew2dd1L2lq8xgZyIHGZCsMBit">click
							here to download all hdf5 data</a>
						</p>

						<li style="list-style:circle; font-size:18px">pickle</li>
						<p>
							Data in hdf5 dataset has been re-organized to transition style and calculated the reward
							inorder to train RL
							algorithm. Every pkl file contain 512 frame of transition data. Each transition contains the
							following data.
							<li style="list-style:circle;margin-left: 40px">mm_scan</li>
							<li style="list-style:circle;margin-left: 40px">laser_scan</li>
							<li style="list-style:circle;margin-left: 40px">pos_diff</li>
							<li style="list-style:circle;margin-left: 40px">action</li>
							<li style="list-style:circle;margin-left: 40px">reward</li>
							<li style="list-style:circle;margin-left: 40px">next_mm_scan</li>
							<li style="list-style:circle;margin-left: 40px">next_laser_scan</li>
							<li style="list-style:circle;margin-left: 40px">next_pos_diff</li>
							<br><br>
							<a href="https://drive.google.com/drive/u/1/folders/1FMkjvJl070_LxqcNBFeBedPsZFoy0VNe">click
								here to download all transition data</a>
						</p>

						<li style="list-style:circle; font-size:18px">All data</li>
						<div class="wrapper-center">
							<iframe
								src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQobaKmi7cr2O4abUyIKbkUFovtQmjz6AFpLUb6LX53qvBen18g5iMI6KiQR9HSEDPOaT6cchKyOi1-/pubhtml?gid=1072056062&amp;single=true&amp;widget=true&amp;headers=false"
								width="611" height="910" frameborder="">
							</iframe>
						</div>
					</ul>
				</h2>

				<h1>Bibtex</h1>
				
				<div 
				style="background-color:rgb(212, 212, 212);
				color:rgb(0, 0, 0); 
				padding:15px;
				width: 680px;
				border: 1px solid rgb(0, 0, 0);;
				margin: 30px;">
				<p>
				@article{huang2021cross, <br>
					title={Cross-Modal Contrastive Learning of Representations for Navigation Using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions}, <br>
					author={Huang, Jui-Te and Lu, Chen-Lung and Chang, Po-Kai and Huang, Ching-I and Hsu, Chao-Chun and Huang, Po-Jui and Wang, Hsueh-Cheng and others}, <br>
					journal={IEEE Robotics and Automation Letters}, <br>
					volume={6}, <br>
					number={2}, <br>
					pages={3333--3340}, <br>
					year={2021}, <br>
					publisher={IEEE} <br>
				}<br>
	
				</p>
			  </div> 


				<div class="clearer">&nbsp;</div>

			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<!-- <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span> -->

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>