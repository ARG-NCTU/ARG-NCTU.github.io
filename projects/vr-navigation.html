<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
					Teleoperating a Heterogeneous Robot Team via Virtual Reality	
					</h0>
					<h2>
						Ching-I Huang<sup>1</sup>,  Sun-Fu Chou<sup>1</sup>, Li-Wei Liou<sup>1</sup>, Nathan Alan Moy<sup>2</sup>,
					</h2>
					<h2 >
						Chi-Ruei Wang<sup>1</sup>, Hsueh-Cheng Wang<sup>1</sup>, Charles Ahn<sup>2</sup>, and Lap-Fai Yu<sup>2</sup>
					</h2>
					
					<h4>________________________________________________</h4>

					<h4>
						<sup>1</sup>Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan.
					</h4>
					<h4>
						<sup>2</sup>Department of Computer Science, George Mason University, USA.
					</h4>
					

				</div>
				<br><br>

				<!-- <div class="wrapper-center">
						<button onclick="window.open('../publications/IROS22_1892_FI_WFH.pdf')"
						class="button button-gray">Paper</button>					
				</div> -->

				<h1>Abstract</h1>
				<p>
					Robots have become essential for hazardous and challenging tasks or those beyond human capabilities. 
					However, the DARPA Subterranean Challenge highlighted that some complex situations still require human input, 
					despite implementation of various autonomous techniques. Additionally, the development and management of 
					heterogeneous multi-robot teams require effective user interfaces to support human interactions. 
					In this paper, we propose an integrated framework that enables users to operate a heterogeneous robot team 
					using commercially available virtual reality (VR) interfaces at a base station. 
					By integrating 3D environment reconstruction and real-world sensor data into the VR interface, 
					users gain a comprehensive real-time understanding of the spatial context from the robot's perspective. 
					We further delved into the application's simulation utility, streamlining its mechanics for faster user adaptation to SAR missions. 
					A comprehensive user study with 53 participants attested to the versatility and efficacy of our framework in facilitating human interactions. 
					Our findings emphatically underscore a preference for: (1) immersive real-time reconstructions within textured meshes over basic real-time reconstructions, 
					(2) multi-user remote control in situations devoid of autonomous capabilities, 
					and (3) superior autonomy for navigation and obstacle negotiation in complex NAMO tasks, as contrasted with traditional goal point navigation techniques. 
				</p>

				<div class="wrapper-center">
					<img src="img/vr_navigation_teaser.png" alt="motivation" height="250">
				</div>

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe title="vimeo-player" src="https://player.vimeo.com/video/865390496?h=b822e45e8d" width="640" height="360" frameborder="0"    allowfullscreen></iframe>
				</div>
				<br>
				
				<h1>System Overview</h1>
				<p>
					Through our integrated framework, we bolster human-robot collaboration using a VR interface, 
					offering immersive, real-time experiences from any robot's first-person view. 
					By harnessing rich spatial information and utilizing an intuitive control panel, 
					users can efficiently reduce workload and craft strategic mission plans. 
					The framework ensures smooth data exchange between the operator's VR platform (Unity) and the robot's operational environment (ROS).

				</p>
				<div class="wrapper-center">
					<img src="img/vr_navi_system.png" height="280" >
				</div>
				<br>

				<h1>Heterogeneous Robot Team</h1>
				<p>
					In this study, two robots were employed: the left with a 360-camera,
					and the right equipped with three depth cameras — two covering a 3m ×
					2m forward area, and the third spanning the robot arm’s full 0.7m stroke
					— along with a LiDAR for ICP-based positioning.
				</p>
				<div class="wrapper-center">
					<img src="img/vr-navi-robots-arm.png" height="280" >
				</div>
				<br>

				<h1>Conclusion</h1>
				<h2>1. An Integrated Framework for Human-Robot Collaboration Leveraging Rich Spatial Information in a VR Interface, with a Prime Focus on Drastically Reducing User Workload. </h2>
				<h2>2. Enhancement of Team Dynamics through Collaborative Human-Robot Interaction and Strengthened Human-to-Human Synergy. </h2>
				<!-- <h2>2. Proposed system allows even <font color="red"> <strong>52 novices</strong> </font> to perform tasks of high dexterity </h2> -->
				<h2>3. Facilitation of high-autonomy navigation in NAMO tasks.</h2>
				<h2>More details you can find in the paper.</h2>
				<br><br><br>
				
				<!-- <h1>Bibtex</h1>
				

				<div 
					style="background-color:rgb(212, 212, 212);
					color:rgb(0, 0, 0); 
					padding:15px;
					width: 680px;
					border: 1px solid rgb(0, 0, 0);;
					margin: 30px;">
					<p>
					@article {WFH-VR , <br>
						author = {Lai Sum Yim, Quang TN Vo, Ching-I Huang, Chi-Ruei Wang, Wren McQueary, Hsueh-Cheng Wang, Haikun Huang and Lap-Fai Yu}, <br>
						title = {WFH-VR: Teleoperating a Robot Arm to set a Dining Table across the Globe via Virtual Reality}, <br>
						journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, <br>
						year = {2022}, <br>
						}
					</p>
				  </div>  -->
				<!-- <h1>Acknowledgments</h1>

				<p>
					The research was supported by Taiwan's National Science and Technology Council (grants 111-NU-E-A49-001-NU, 110-2221-E-A49-124, and 111R10093Y-2). 
					This work was funded in part by Qualcomm through the Taiwan University Research Collaboration Project. 
					This work was supported by an NSF CAREER award (award\#: 1942531) and an NSF FTW-HTF-R grant (award\#: 2128867).
				</p> -->


				<div class="wrapper-center">
					<img src="img/vr-final.PNG" height="150" >
				</div>


				<br><br><br>


			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<!-- <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span> -->

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>
