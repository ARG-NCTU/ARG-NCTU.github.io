<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
	<meta name="description" content="description" />
	<meta name="keywords" content="keywords" />
	<meta name="author" content="author" />
	<style>
		table {
			font-family: arial, sans-serif;
			border-collapse: collapse;
			width: 100%;
		}

		td,
		th {
			border: 1px solid #dddddd;
			padding: 8px;
		}

		tr:nth-child(even) {
			background-color: #cdcdcd;
		}
	</style>
	<link rel="stylesheet" type="text/css" href="../default.css" media="screen" />
	<title>Nick Wang at NCTU</title>
</head>

<body>

	<div class="outer-container">

		<div class="inner-container">

			<div class="header">

				<div class="title">
					<span class="sitename"><a href="../index.html">Assistive Robotics Group</a>&nbsp;&nbsp;&nbsp;<a
							href="../index_ch.html">機器人與輔助科技實驗室</a></span>
				</div>

			</div>

			<div class="path">

				<a href="../index.html">Home</a>
				<a href="../event.html">News</a>
				<a href="../people.html">People</a>
				<a href="../robots.html">Robots</a>
				<a href="../research.html">Research</a>
				<a href="../publications.html">Publications</a>
				<a href="../courses.html">Courses</a>
				<a href="../materials.html">Materials</a>
			</div>

			<div class="blank">
				<div class="wrapper-center">
					<h0 style="text-align: center">
					An Evaluation Framework of Human-Robot Teaming for Navigation among Movable Obstacles via Virtual Reality-based Interactions	
					</h0>
					<h2>
						Ching-I Huang<sup>1</sup>,  Sun-Fu Chou<sup>1</sup>, Li-Wei Liou<sup>1</sup>, Nathan Alan Moy<sup>2</sup>,
					</h2>
					<h2 >
						Chi-Ruei Wang<sup>1</sup>, Hsueh-Cheng Wang<sup>1</sup>, Charles Ahn<sup>2</sup>, Chun-Ting Huangand<sup>3</sup>, Lap-Fai Yu<sup>2</sup>
					</h2>
					
					<h4>________________________________________________</h4>

					<h4>
						<sup>1</sup>Institute of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan.
					</h4>
					<h4>
						<sup>2</sup>Department of Computer Science, George Mason University, USA.
					</h4>
					<h4>
						<sup>3</sup>Qualcomm Inc., USA
					</h4>
					

				</div>
				<br><br>

				<!-- <div class="wrapper-center">
						<button onclick="window.open('../publications/IROS22_1892_FI_WFH.pdf')"
						class="button button-gray">Paper</button>					
				</div> -->

				<h1>Abstract</h1>
				<p>
					Robots are essential for tasks that are hazardous
					or beyond human capabilities. However, the results of the
					Defense Advanced Research Projects Agency (DARPA) Sub-
					terranean (SubT) Challenge revealed that despite various tech-
					niques for robot autonomy, human input is still required in
					some complex situations. Moreover, heterogeneous multirobot
					teams are often necessary. To manage these teams, effective user
					interfaces to support humans are required. Accordingly, we
					present a framework that enables intuitive oversight of a robot
					team through immersive virtual reality (VR) visualizations. The
					framework simplifies the management of complex navigation
					among movable obstacles (NAMO) tasks, such as search-and-
					rescue tasks. Specifically, the framework integrates a simulation
					of the environment with robot sensor data in VR to facilitate
					operator navigation, enhance robot positioning, and greatly
					improve operator situational awareness. The framework can
					also boost mission efficiency by seamlessly incorporating au-
					tonomous navigation algorithms, including NAMO algorithms,
					to reduce detours and operator workload. The framework is
					effective for operating in both simulated and real scenarios and
					is thus ideal for training or evaluating autonomous navigation
					algorithms. To validate the framework, we conducted user
					studies (N = 53) on the basis of the DARPA SubT Challenge’s
					search-and-rescue missions. 
				</p>

				<div class="wrapper-center">
					<img src="img/vr_navigation_teaser.png" alt="motivation" height="250">
				</div>

				<h1>Video</h1>
				<p></p>
				<div class="container" align="middle">
					<iframe title="vimeo-player" src="https://player.vimeo.com/video/865390496?h=b822e45e8d" width="640" height="360" frameborder="0"    allowfullscreen></iframe>
				</div>
				<br>
				
				<h1>System Overview</h1>
				<p>
					Through our integrated framework, we bolster human-robot collaboration using a VR interface, 
					offering immersive, real-time experiences from any robot's first-person view. 
					By harnessing rich spatial information and utilizing an intuitive control panel, 
					users can efficiently reduce workload and craft strategic mission plans. 
					The framework ensures smooth data exchange between the operator's VR platform (Unity) and the robot's operational environment (ROS).

				</p>
				<div class="wrapper-center">
					<img src="img/vr_navi_system.png" height="280" >
				</div>
				<br>

				<h1>Heterogeneous Robot Team</h1>
				<p>
					Two robots were used for testing. Jackal (left) with a 360° camera
					and Husky (right) with an arm (UR5) and three depth cameras. Two of these
					cameras covered a 3 m × 2 m forward area, and the third spanned the
					arm’s full 0.7-m stroke. Husky also had a LiDAR for ICP positioning.
				</p>
				<div class="wrapper-center">
					<img src="img/vr-navi-robots-arm.png" height="280" >
				</div>
				<a href="https://clearpathrobotics.com/jackal-small-unmanned-ground-vehicle/">Jackal,  Clearpath Robotics, Inc., Canada </a>
				<br>
				<a href="https://clearpathrobotics.com/husky-unmanned-ground-vehicle-robot/">Husky,  Clearpath Robotics, Inc., Canada </a>
				<br>
				<a href="https://www.universal-robots.com/products/ur5-robot/">UR5,  Universal Robots A/S, Inc, Denmark </a>
				<br>
				<br>
				<p>
				The Adaptivity of Different Robot
				</p>
				<div class="container" align="middle">
					<iframe src="https://player.vimeo.com/video/893656641?h=f41349f858" width="640" height="372" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
					<p><a href="https://www.trossenrobotics.com/locobot-pyrobot-ros-rover.aspx">LoCoBot WX20, Trossen Robotics, Inc., Illinois, U.S. </a> </p>
				</div>
				<br>

				<h1>Appendix A</h1>
				<p>
					The 3D model of the environment is captured using a Matterport camera, featuring a tripod-mounted camera rig equipped with three color and three depth cameras strategically oriented
					—slightly up, horizontal, and slightly down. During each panorama capture, the camera rig rotates around the direction of gravity, stopping at six distinct orientations. 
					At each stop, the camera captures an HDR photo from each of the three RGB cameras, while the three depth cameras continuously acquire data throughout the rotation. 
					The collected depth data is then integrated to synthesize a 1280x1024 depth image aligned with each color image. 
					For each panorama, the result is 18 RGB-D images with nearly coincident centers of projection at the height of a human observer. 
					In this study, a single operator recorded our 3D mesh model in approximately one and a half hours. 
					Note that the Trojan Nuclear Power Plant reconstruction is provided by DARPA, and auditorium is a sample model from Matterort Inc.
				</p>
				<div class="wrapper-center">
					<img src="img/3Dreconstruction.png" height="230" >
					<p>
					<a  href="https://my.matterport.com/show/?m=eh1MbMR14xC&fbclid=IwAR15Umv7rn71GtL7RtWWDtIvi62DzgvAQKUABNL-I1Wf1u1p_ut-DckbcBU"> auditorium (conference center in Munich, Germany) from Matterort Inc.</a>
					<br>
					<a  href="https://my.matterport.com/show/?m=Y5R149J3Efn"> Campus Building at NYCU in Taiwan </a>
					<br>
					<a  href="http://gofile.me/773h8/U5Gr0iENO"> Free to download Campus Building at NYCU in Taiwan </a>
					<a  href="http://gofile.me/773h8/IT92nJaIC"> Free to download all models and related Gazabo world </a>
					<br>
					<br>
					<a href="https://support.matterport.com/s/article/Matterport-Pro2-3D-Camera-Specifications?language=en_US&categfilter=Matterport_Pro2&parentCategoryLabel=Matterport_Pro_Cameras">
					Matterport Pro2 3D Camera, Matterport, Inc., California, U.S. </a> </p>
				</div>
				<br>


				<h1>Appendix B</h1>
				<p>
					Tutorial: how to build up the environment for teleoperation .
					<br>
					<a  href="https://github.com/ARG-NCTU/oop-proj-unity-ros/tree/master"> ROS</a>
					<br>
					<a  href="https://github.com/ARG-NCTU/oop-proj-unity?fbclid=IwAR2kkBxrc9BbTJNpASYLHRGR-uvg9Mlw90COTCPu2QLaVx4-wo33IgKAy9Q"> Unity</a>
					<br>
				</p>

				<br>

				<h1>Appendix C</h1>
				<p>
					Repo for reconstruction of point clouds by  RGB and D images in Unity platform.
					<br>
					<a  href="https://github.com/ARG-NCTU/unity-PointCloudStreaming"> Point-cloud streaming (PCS)</a>
					<br>
					<div class="wrapper-center">
					<img src="img/pcs.png" height="280" >
				</div>
				</p>

				<br>


				<h1>Conclusion</h1>
				<h2>1. An Integrated Framework for Human-Robot Collaboration Leveraging Rich Spatial Information in a VR Interface, with a Prime Focus on Drastically Reducing User Workload. </h2>
				<h2>2. Enhancement of Team Dynamics through Collaborative Human-Robot Interaction and Strengthened Human-to-Human Synergy. </h2>
				<!-- <h2>2. Proposed system allows even <font color="red"> <strong>52 novices</strong> </font> to perform tasks of high dexterity </h2> -->
				<h2>3. Facilitation of high-autonomy navigation in NAMO tasks.</h2>
				<h2>More details you can find in the paper.</h2>
				<br><br><br>
				
				<!-- <h1>Bibtex</h1>
				

				<div 
					style="background-color:rgb(212, 212, 212);
					color:rgb(0, 0, 0); 
					padding:15px;
					width: 680px;
					border: 1px solid rgb(0, 0, 0);;
					margin: 30px;">
					<p>
					@article {WFH-VR , <br>
						author = {Lai Sum Yim, Quang TN Vo, Ching-I Huang, Chi-Ruei Wang, Wren McQueary, Hsueh-Cheng Wang, Haikun Huang and Lap-Fai Yu}, <br>
						title = {WFH-VR: Teleoperating a Robot Arm to set a Dining Table across the Globe via Virtual Reality}, <br>
						journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, <br>
						year = {2022}, <br>
						}
					</p>
				  </div>  -->
				<!-- <h1>Acknowledgments</h1>

				<p>
					The research was supported by Taiwan's National Science and Technology Council (grants 111-NU-E-A49-001-NU, 110-2221-E-A49-124, and 111R10093Y-2). 
					This work was funded in part by Qualcomm through the Taiwan University Research Collaboration Project. 
					This work was supported by an NSF CAREER award (award\#: 1942531) and an NSF FTW-HTF-R grant (award\#: 2128867).
				</p> -->


				<div class="wrapper-center">
					<img src="img/vr-final.PNG" height="150" >
				</div>


				<br><br><br>


			</div>

			<div class="footer">

				<span class="left">
					&copy; H.C. Wang
				</span>

				<!-- <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a
						href="http://arcsin.se/">Arcsin</a></span> -->

				<div class="clearer"></div>

			</div>

		</div>

	</div>

</body>

</html>